\documentclass{llncs}

\usepackage[latin1]{inputenc}
\usepackage{epsfig}
\usepackage{graphicx}        % standard LaTeX graphics tool
\usepackage{url}
\usepackage{subfig}
\usepackage{calc}
\usepackage{makeidx}
%\usepackage{amstext}
 \usepackage{amsmath}
 
\usepackage{multicol}
\usepackage{pslatex}
\usepackage{caption}  
\usepackage{amssymb}
\usepackage{verbatim}
\usepackage{algorithm,algorithmic}


\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   TITLE   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{An Improved Decision System for URL Accesses based on a Rough Feature Selection Technique} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   AUTHORS   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\author{P. de las Cuevas, Z. Chelly, A.M. Mora, J.J. Merelo, A.I. Esparcia-Alc\'azar}
\authorrunning{P. de las Cuevas et al.}

\institute{Department of Computer Architecture and Computer Technology, University of Granada, Spain. \\
Laboratoire de Recherche Op\'erationelle de D\'ecision et de Contr\^ole de Processus, Institut Sup\'erieur de Gestion, Tunisia.\\
S2 Grupo, Spain\\
% problem with letters here. You should probably either use LaTeX symbols for that or change to UTF8 - JJ
% Ok, final version will be stored with the appropriate encoding - Paloma
% Antonio - I'm fixing the symbols. Zeinebtry to change your editor configuration.
 {\tt \{paloma,amorag,jmerelo\}@geneura.ugr.es, zeinebchelly@yahoo.fr, aesparcia@s2grupo.es}
}

\maketitle

%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   ABSTRACT   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\begin{abstract} 
Corporate security is usually one of the matters in which companies invest more resources, since the loss of information directly translates into money loss; these losses might have an origin in external attacks or internal security failures, but an important part of the security breaches is related to the lack of awareness that the employees have with regard to the use of the Web. In this work we have focused on the latter problem, creating and improving a system
able to detect anomalous and potentially insecure situations that could be dangerous for a company. This system was initially conceived as a step beyond the use of what are known as black/white lists. These lists contain URLs that are forbidden to access, or dangerous (black lists), or URLs to which the access is permitted or allowed (white lists). 
In this chapter, we propose a system that can initially learn from the already known black/white lists, and then classify a new, unknown, URL connection either as ``should be allowed'' or ``should be denied''. This system is described, as well as its results, and the improvements made by means of an initial data pre-processing 
% Antonio - we have used the term 'preprocessing' in several places in the paper. Please fix it (here or there) to use always the same. ;)
% We are changing the rest to pre-processing.
step based on applying Rough Set Theory for feature selection. We prove that 
high accuracies can be obtained with a proper pre-processing of the data, reaching between 96\% and 97\% of correctly classified patterns. We also demonstrate that including the use of Computational Intelligence techniques, such as the one used for feature selection, enhances the system performance, while the accuracies maintain their values.
Indeed, among the obtained results, we demonstrate that it is possible to obtain interesting rules which are not based only on the URL string feature, for classifying new unknown URLs access requests as allowed or as denied. 
\end{abstract}
% Antonio - I think that the asbtract is a bit long.

\textbf{Keywords.} Computational Intelligence, Rough Sets, Feature Selection, Corporate Security Policies, Internet Access Control, Data Mining, Blacklists and Whitelists.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   INTRODUCTION   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{Introduction}
\label{sec:introduction}

\noindent Security is an inclusive term that refers to a diversity of steps taken by individuals, 
and companies, in order to protect computers or computer networks that are connected to the Internet. 
The Internet was initially conceived as an open network facilitating the free exchange of information. 
However, data which is sent/received over the Internet travel through a dynamic chain of computers and 
network links and, as a consequence, the risk of intercepting and changing the data is high. 
In fact, it would be virtually impossible to secure every computer connected to the Internet around the world. 
So, there will likely always be weak links in the chain of data exchange \cite{cheswick2003firewalls}. 
Yet, companies have to find out a way for their employees to safely interact with customers, clients, and anyone 
who uses the Internet while protecting internal confidential information. Companies have, also, to alert the employees 
from the Internet misuse while doing their job.
 
Most of the time, employees have a misguided sense of security and believe that it is an IT problem, a purely technical 
issue, and they naively believe that an incident may never happen to them \cite{stanton2005analysis}.  
Actually, the employees' web misuse is one of the main causes of  security breaches \cite{breivik2002abstract},  
so that making them security-conscious has become a security  challenge. 

The reality is that every department must be involved in  readiness planning and establishing security policies 
and procedures  to minimize their risks. Such strategies are mainly handled by 
means  of \textit{Corporate Security Policies} (CSPs) which basically are a  set of security rules aiming at 
protecting company assets by defining  permissions to be considered for every different action to be  performed 
inside the security system \cite{kaeo2003designing}.
 
The basic idea behind these CSPs is usually to include rules to either allow or deny employees' access to 
non-confident or non-certified websites, which are referenced by their URLs in this chapter. Moreover, several web pages 
might be also controlled for productivity  or suitability reasons, given the fact that the employees who connect  
to these might have working purposes or not. Actually, some of the  CSPs usually define sets of allowed or denied 
web pages or websites that could be accessed by the company employees. These sets are  usually included in two main 
lists; a white list (referring to  ``permitted'') and a black list (referring to  ``non-permitted''). 
Both lists, the white and the black, act as a  good and useful control tools for those URLs included in them, 
as  well as for the complementary. For instance, the URLs which are not included in a white list will automatically 
have a denial of  access \cite{ludl2007effectiveness}.

The aim of this paper is going beyond this traditional and simple decision making process. Actually, by using black 
and/or white lists, we either allow or deny users' requests/connection based, only, on the URLs provided in the lists. 
Yet, updating these lists is a never ending task, as numerous malicious websites appear every day. 
For instance, Netcraft reports from November of 2014 \cite{netcraft:site} showed that there are about 950 
million active websites. But McAfee reported \cite{mcafee:site} that, at the end of the first quarter of 2014, 
there were more than 18 million new suspect URLs (2 million associated domains), and also more than 250 thousand 
new phishing URLs (almost 150 thousand associated domains).

With this situation in mind, in this chapter, our aim is to define a tool for automatically making allow or 
deny decisions with respect to URLs that are, actually, not included in the aforementioned lists. This decision 
would be based on that made for similar URL accesses (those with similar features), but instead of using only 
the URL strings included in the lists, we will consider other parameters of the request/connection.

For this reason, the problem has been mapped to a \textit{classification} problem in which we start from a set of 
unlabelled patterns that model the connection properties from a huge amount of \textit{actual}\footnote{Taken from a 
log file released to   us by a  Spanish company.} URL accesses, known as sessions. After that, we assign a label to many 
of them, considering a set of \textit{actual}\footnote{The set of rules has been written by the same   company, with respect 
to its employees.} security rules (CSPs) defined by the Chief Security Officer (CSO) in the company. This was the approach 
followed in \cite{ECTA}, and which we extend in this chapter.

In order to extract conclusions from the resulting studied dataset and to properly apply a classification algorithm, 
a pre-processing step is needed. In fact, to obtain an accurately trained classifier, there is a need to extract as much 
information as possible from the connections
that the employees normally make throughout the workday. This translates into high computational requirements, which is 
why we introduce in this paper  techniques for data reduction. More precisely, we aim to apply a feature selection 
technique to extract the most important features from the data at hand. Among the well known feature selection techniques 
proposed in literature, we propose to use a Computational Intelligence method: the Rough Set Theory (RST) \cite{pawlak2008rough}. RST has been experimentally 
evaluated with other leading feature selection techniques, such as Relif-F and entropy-based approaches 
in \cite{jensen2007fuzzy}, and has been shown to outperform these in terms of resulting classification performance.
 
After pre-processing and based on the reduced dataset, we will apply several classification algorithms, 
testing them and selecting the most appropriate one for this problem. The selected classifier should be capable 
of dealing with our data while producing high accuracies and
being lightweight in terms of running time. Moreover, as we want to further test the reliability of the results, 
in this work we propose different experimental setups based on different data
partitions. These partitions are formed either by preserving the order of the data or by taking the patterns in 
a random way. Finally, given that the used data presents unbalance, we aim to apply balancing 
techniques \cite{imbalance_techniques_02} to further guarantee the fairness of our obtained results.

In this chapter, we want to improve the accuracies obtained in our previous work \cite{ECTA}, 
as well as see if the new incorporated method (namely Rough Sets) for feature selection yields to better rules. 
This is meant to be done not only by applying RST for feature selection, but also by improving the quality of the 
original data set, by means of erasing information that may be redundant.

The rest of the paper is structured as follows. Next section describes the state of the art related 
to Data Mining (DM), Machine Learning (ML), and Computational Intelligence (CI) techniques applied 
to corporate security. Also, related works about URL filtering will be reviewed. Data  description is 
detailed in Section \ref{sec:problemDescription}. Then, Section \ref{sec:featureselection} describes the  
basic concepts of Rough Set Theory for feature selection which we  have used for data pre-processing. 
Section \ref{sec:methodology} gives an overview of the  followed methodology, as well as the improvements 
done after our  first results obtained in \cite{ECTA}. Results are detailed in Section \ref{sec:results}. 
Then, Section \ref{sec:rulesdiscussion}   discusses the  obtained rules which are different for every used 
classifier. Finally, conclusions and future trends are given in Section \ref{sec:conclusions}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  STATE OF THE ART  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{State of the Art}
\label{sec:stateofart}

\noindent Our work tries to obtain a URL classification tool for enhancing the security in the client side, as at the end we want to get if a certain URL is secure or not, having as reference a set of rules (derived from a CSP) which allow or deny a set of known \textit{HTTP} requests. For this, different techniques belonging to Data Mining (DM), Machine Learning (ML), and Computational Intelligence, have been applied. This section gives  an overview in a number of solutions given to protect the user, or the company, against insecure situations. 

% ------------------------------------------------------------------
\subsection{Data Analysis and pre-processing}
\label{subsec:dataanalysis}

Performing DM means that an analysis is performed over the database we have \cite{Frank2011}. In our case, this database is a log of URL connections. Leake \& Wilson  \cite{wilson2001maintaining} present a vast review of works which study database maintenance, and the conclusion is that a database with good quality is decisive when trying to obtain good accuracies; a fact which was also demonstrated in \cite{zeineb2014thesis}. To analyse the data that we have at hand, we have based our work on two main processes. A process consisting of performing data pre-processing on the URL dataset and a process consisting of applying balancing techniques depending on the data at hand. 

While performing data pre-processing, we have focused, first , on the kind/type of data included in the URL connections log file; the input file. We remarked that many URL strings are redundant in the dataset and thus we aimed to maintain the input file by eliminating the redundant strings. Actually, the quality of the dataset is very important to generate accurate results and to have the possibility to learn models from the presented data. Thus, maintaining it is an essential step. Many maintenance techniques were proposed in literature \cite{wilson2001maintaining} in order to guarantee the good quality of a given dataset.   Most of these techniques are based on updating a database by adding or deleting instances to optimize and reduce the initial database. 
These policies include different operations such as: the outdated, redundant, or inconsistent instances may be deleted; groups of objects may be merged to eliminate redundancy and improve reasoning power; objects may be re-described to repair incoherencies; signs of corruption in the database have to be checked, and any abnormalities in the database which might signal a problem has to be controlled. A database which is not maintained can become sluggish and without accurate data users will make uninformed decisions. In this work, we have maintained the URL dataset that we have at hand by focusing on a specific kind of data that should be eliminated; which are the redundant URL strings. Section \ref{subsec:duplicateddata} explains in detail the process that we have adopted to eliminated these redundant data.

Still with the data pre-processing task, we have focused as a second step on the set of features presented in the URL connections log file. Thus we tried to select the most informative features from the initial feature set. At this point, we have introduced an extra technique, a data reduction technique, that was not included in our first work presented in \cite{ECTA}. 

Actually, data reduction is a main point of interest across a wide variety of fields. In fact, focusing on this step is crucial as it often presents a source of significant data loss. Many techniques were proposed in literature to achieve the task of data reduction and they can be categorized into two main heads; techniques that transform the original meaning of the features, called the ``transformation-based approaches", and the second category is a set of semantic-preserving techniques known as the ``selection-based approaches".

Transformation based approaches, also called ``feature extraction approaches", involve simplifying the amount of resources required to accurately describe a large set of data. Feature extraction is a general term for methods that construct combinations of variables to represent the original set of features but with new variables while still describing the data with sufficient accuracy. The transformation based techniques are employed in situations where the semantics of the original database will not be needed by any future process. In contrast to the semantics-destroying dimensionality reduction techniques, the semantics-preserving techniques, also called ``feature selection techniques", attempt to retain the meaning of the original feature set. The main aim of this kind of techniques is to determine a minimal feature subset from a problem domain while retaining a suitably high accuracy in representing the original features \cite{liu1998feature}. In this work, we mainly focus on the use of a feature selection technique, instead of a feature extraction technique, as it is crucial to preserve the semantics of the features in the URL data that we dispose at hand, and among them, select the most important/informative ones which nearly preserve  the same performance as the initial feature set.

Yet, it is important to mention that most of the feature selection techniques, proposed in literature, suffer from some limitations. Most of these techniques involve the user for the task of the algorithms parameterization and this is actually seen as a significant drawback. Some feature selectors require noise levels to be specified by the user beforehand, some simply rank features leaving the user to choose their own subset. There are those that require the user to state how many features are to be chosen, or they must supply a threshold that determines when the algorithm should terminate. All of these require the user to make a decision based on its own (possibly faulty) judgment \cite{jensen2005semantics}. To overcome the shortcomings of the existing methods, it would be interesting to look for a method that does not require any external or additional information to function appropriately. Rough Set Theory (RST) \cite{pawlak2008rough}, which will be deeply explained in Section \ref{sec:featureselection}, can be used as such tool. 
 
As previously stated and apart from applying a data pre-processing process, we aim to apply balancing techniques, depending on the distribution of patterns per class, in order to ensure the fairness of our results. This is due to the fact that using ``real data''\footnote{Data which was gathered from the real world, and was not artificially generated.} may yield to highly unbalanced data sets \cite{imbalanced_data_05}. This is our case, as the log file includes a set of URL accesses performed by humans, and indeed we obtained an unbalanced dataset. In order to deal with this kind of data there exist several methods in literature known as balancing techniques \cite{imbalanced_data_05}. These methods can be categorized into three main groups \cite{imbalance_techniques_02}: 

\begin{itemize}
\item \textit{Undersampling the over-sized classes}: This category aims at reducing the considered number of patterns for the classes with the majority.
\item \textit{Oversampling the small classes}: This category aims at introducing additional (normally synthetic) patterns in the classes with the minority.
\item \textit{Modifying the cost associated to misclassifying the positive and the negative class}: This category aims at compensating the unbalance in the ratio of the two classes. For example, if the imbalance ratio is 1:10 in favour of the negative class, the penalty of misclassifying a positive example should be 10 times greater.
\end{itemize}

Techniques belonging to the first group have been applied to some works, following a random undersampling approach \cite{random_undersampling_08}. However, those techniques have the problem of the loss of valuable information. 

Techniques belonging to the second  group have been so far the most widely used, following different approaches, such as SMOTE (Synthetic Minority Oversampling Technique) \cite{smote_02} which is a method proposed by Chawla et al. for creating `artificial' samples for the minority class in order to balance the amount of them, with respect to the amount of samples in the majority class. However this technique is based on numerical computations, considering  different distance measures, in order to generate useful patterns  (i.e. realistic or similar to the existing ones).

The third group implies using a method in which a cost can be associated to the classifier accuracy at every step. This was done for instance by Alfaro-Cid et al. in \cite{cost_adjustment_07}, where they used a Genetic Programming (GP) approach in which the fitness function was modified in order to consider a penalty when the classifier makes a false negative (an element from the minority class was classified as belonging to the majority class).
However almost all the approaches deal with numerical (real, integer) data.

Details about the balancing techniques used in our work will be explained in Section \ref{sec:methodology}. 

% ------------------------------------------------------------------
%
\subsection{Related works and contribution}
\label{subsec:relatedworks}

The works that we are interested in are those which scope is related with the users' information and behaviour, and the management (and adaptation) of Information or Corporate Security Policies (ISPs).

In this line, the paper proposed by Greenstadt and Beal \cite{cognitive_security_08} combined biometrics signals with ML methods in order to get a reliable user authentication in a computer system. P.G. Kelley et al. \cite{user-controllable_learning_08} presented a method named \textit{user-controllable policy learning} in which the user gives feedback to the system every time that a security policy is applied, so these policies can be refined according to that feedback to be more accurate with respect to the user's needs. 

On the other hand, policies could be created for enhancing user's privacy, as proposed by Danezis in \cite{inferring_policies_socialnetworks_09}, who defined a system able to infer privacy-related restrictions by means of a ML method applied in a social network environment. The idea of inferring policies can be also considered after our results, given the fact that we are able to obtain new rules from the output of the classifiers, but in the scope of the company, and focused on ISPs.

In the same line, Lim et al. proposed a system \cite{sec_policy_evolution_gp_08,pol_evol_gp_3_approaches_08} which evolves a set of computer security policies by means of GP, taking again into account the user's feedback. Furthermore, Suarez-Tangil et al. \cite{rule_generation_gp_09} took the same approach as Lim et al., but also bringing event correlation into it. The two latter works are interesting in our case, though they are not focused on company ISPs - for instance, our case with the allowed or denied http requests -.

Furthermore, it is worth mentioning a tool developed in Perl language, by Evan Harris \cite{harris2003next}, taking the approach of ``greylisting'', and which \textit{temporarily} rejects messages that come from senders who are not in the black list or in the white list, so that the system does not know if it is a spam message or not. And, like in our approach, it works trying to have a minimal impact on the users.

Finally, a system named MUSES (from Multiplatform Usable Endpoint Security System) \cite{MUSES_SAC_14} is being developed under the European Seventh Framework  programme (FP7). This system will include event treatment on the user actions inside a company, DM techniques for applying the set of policies from the company ISP to the actions, allowing or denying them, CI techniques for enhancing the system performance, and ML techniques for improving the set of rules derived from these policies, according to user's feedback and behaviour after the system decisions \cite{muses_sotics_13}. The results of this work could be applied in this system, by changing the pre-processing step, due to the fact that the database is different. But overall, our conclusions can be escalated to be included in such a system.   

%Introduce next section - JJ
In the next Section, we will describe the problem we aim to solve, in addition to the data from which the data sets are composed.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%% PROBLEM DESCRIPTION %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%

\section{Problem and Data Description} 
\label{sec:problemDescription}

\noindent The problem to solve is related with the application of corporate security policies in order to deal with potential URL accesses inside an enterprise. To this end a dataset of URL sessions (requests and accesses) is analysed. These data are labelled with the corresponding permission or not for that access following the aforementioned rules. The problem is then transformed into a classification one, in which every new URL request will be classified, and thus, a grant or deny action will be assigned to that pattern.

The analysed data come from an \texttt{access.log} of the Squid
proxy application
\cite{squid:site}, in a real Spanish company. This open source tool
works as a proxy, but with the advantage of storing a cache of recent
transactions so future requests may be answered without asking the
origin server again \cite{DuaneWessels2004}.  
Every pattern, namely a URL request has ten variables associated,
which we describe in Table \ref{tabdata}, indicating if the variable
is numeric or nominal/categorical. The table has, however, not only ten but eleven described variables. This is due to the fact that we decided to consider the `Content Type' of the requested webpage as a whole, but also its Main Content Type (MCT) separately. By adding more information through a new feature, we intended to see if more general rules could be obtained by the classifiers, given that there are less possible values for an MCT than for a whole `Content Type'.

\begin{table*}[htpb]
\centering
 \caption{\label{tabdata} Independent Variables corresponding to a URL request through \textit{HTTP}. The URLs are parsed as detailed in Subsection \ref{subsec:logparsing}.}
{\scriptsize
\begin{tabular}{llll}
\hline\noalign{\smallskip}
Variable name & Description & Type & Rank/Number of Values (if categorical)\\
\noalign{\smallskip}\hline\noalign{\smallskip}
\texttt{http\_reply\_code} & Status of the server response & Categorical & 20 values\\
\texttt{http\_method} & Desired action to be performed & Categorical & 6 values\\
\texttt{duration\_milliseconds} & Session duration & Numerical & integer in [0,357170]\\
\texttt{content\_type} & Media type of the entity-body sent to the recipient & Categorical & 85 values (whole content)\\
\texttt{content\_type\_MCT} & \textbf{M}ain \textbf{C}ontent \textbf{T}ype of the media type. & Categorical & 11 values\\
\texttt{server\_or\_cache\_address} & IP address & Categorical & 2343 values\\
\texttt{time} & connection hour (in the day) & Date & 00:00:00 to 23:59:59\\
\texttt{squid\_hierarchy} & It indicates how the next-hop cache was selected & Categorical & 3 values\\
\texttt{bytes} & Number of transferred bytes during the session & Numerical & integer in [0,85135242]\\
\texttt{client\_address} & IP address & Categorical & 105 values\\
\texttt{URL} & Core domain of the URL, not taking into account the TLD & Categorical & 976 values\\
\noalign{\smallskip}\hline
\end{tabular}
}
\end{table*}

The dependent variable or class is a label which inherently assigns an decision (and so the following action) to every request. This can be: \textit{ALLOW} if the access is permitted according to the CSPs, or can be \textit{DENY}, if the connection is not permitted. These patterns are labelled using an `engine' based in a set of security rules, that specify the decision to make. This process is described in Subsection \ref{subsec:ruleparsing}.

These data were gathered along a period of two hours, from 8.30 to
10.30 am (30 minutes after the work started), monitoring the activity
of all the employees in a medium-size Spanish company (80-100 people),
obtaining 100000 patterns. We consider this dataset as quite complete because it contains a very diverse amount of connection patterns, going from personal (traditionally addressed at the first hour of work) to professional issues (the rest of the day).
Moreover, the results derived from the experiments (described in Section \ref{sec:results}) show that this quantity of data might be big enough, but a more accurate outcome would be given with, for instance, a 24 hours long log.

After describing the whole followed methodology, an overview of feature selection techniques, and more specifically Rough Set Theory applied to them, is depicted in the nect Section.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   ROUGH SET FOR FEATURE SELECTION  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{Rough Set Based Approach for Feature Selection}
\label{sec:featureselection}

Data reduction is a main point of interest across a wide variety of fields. In fact, focusing on this step is crucial as it often presents a source of significant data loss. Many techniques were proposed in literature to achieve the task of data reduction and they can be categorized into two main heads; techniques that transform the original meaning of the features, called the ``transformation-based approaches", and the second category is a set of semantic-preserving techniques known as the ``selection-based approaches".

Yet, it is important to mention that most of the feature selection techniques, proposed in literature, suffer from some limitations. Most of these techniques involve the user for the task of the algorithms parameterization and this is actually seen as a significant drawback. Some feature selectors require noise levels to be specified by the user beforehand, some simply rank features leaving the user to choose their own subset. There are those that require the user to state how many features are to be chosen, or they must supply a threshold that determines when the algorithm should terminate. All of these require the user to make a decision based on its own (possibly faulty) judgment \cite{jensen2005semantics}. To overcome the shortcomings of the existing methods, it would be interesting to look for a method that does not require any external or additional information to function appropriately. Rough Set Theory (RST) \cite{pawlak2008rough} can be used as such tool. In this Section, we briefly discuss the RST main concepts dedicated for feature selection.

\subsection{Preliminaries of Rough Set Theory}

Data are represented as a table where each row represents an object and where each column represents an attribute
that can be measured for each object. Such table is called an ``Information System" (IS). Formally, an IS can be defined as a pair $IS = (U, A)$ where $U = \{x_1, x_2, \ldots, x_n\}$  is a non-empty, finite set of objects called the \emph{``universe"} and $A = \{a_1, a_2, \ldots, a_k\}$ is a non-empty, finite set of attributes. Each attribute or feature $a \in A $ is associated with a set $V_a$ of its value, called the \emph{domain} of $a$. We may partition the attribute set $A$ into two subsets $C$ and $D$, called   \emph{condition} and \emph{decision} attributes, respectively \cite{pawlak2008rough}.


Let $P \subset A$ be a subset of attributes. The indiscernibility relation, denoted by $IND(P)$, is an equivalence relation defined as: $IND(P) =  \{(x, y) \in U \times U \mbox{ : } \forall a \in P, a(x) = a(y)\},$ where $a(x)$ denotes the value of feature $a$ of object $x$. If $(x, y) \in IND(P)$, $x$ and $y$ are said to be \emph{indiscernible} with respect to $P$. The family of all equivalence classes of $IND(P)$ (Partition of $U$ determined by $P$) is denoted by $U/IND(P)$. Each element
in $U/IND(P)$ is a set of indiscernible objects with respect to $P$. Equivalence classes $U/IND(C)$ and $U/IND(D)$
are called \emph{condition} and \emph{decision} classes.


For any concept $X \subseteq U$ and attribute subset $R \subseteq A$, $X$ could be approximated by the R-\emph{lower} approximation and R-\emph{upper} approximation using the knowledge of $R$. The lower approximation of $X$ is the set of objects of $U$ that are surely in $X$, defined as: $ \underline{R}(X) = \bigcup \{E \in U/IND(R)  : E \subseteq X \}.$ The upper approximation of $X$ is the set of objects of $U$ that are possibly in $X$, defined as: $\overline{R}(X) = \bigcup \{E \in U/IND(R)  : E \cap X \ne \emptyset \}$. The boundary region is defined as:

\begin{displaymath}
BND_R(X) = \overline{R}(X) - \underline{R}(X)
\end{displaymath}

If the boundary region is empty, that is, $\overline{R}(X) = \underline{R}(X)$, concept $X$ is said to be R-\emph{definable}. Otherwise $X$ is a rough set with respect to $R$.

The positive region of decision classes $U/IND(D)$ with respect to condition attributes $C$ is denoted by $POS_c(D)$ where:

\begin{displaymath}
POS_c(D) = \bigcup \overline{R}(X)
\end{displaymath}

The positive region $POS_c(D)$ is a set of objects  of $U$ that can be classified with certainty to classes $U/IND(D)$ employing attributes of $C$. In other words, the positive region $POS_c(D)$ indicates the union of all the equivalence classes defined by $IND(P)$ that each for sure can induce the decision class $D$.

\subsection{Reduction Process}

The aim of feature selection is to remove unnecessary features to the target concept. It is the process of finding a smaller set of attributes, than the original one, with the same or close classification power as the original set. Unnecessary features, in an information system, can be classified into irrelevant features  that do not affect the target concept in any way  and redundant (superfluous) features that do not add anything new to the target concept.

RST for feature selection is based on the concept of  discovering dependencies between attributes. Intuitively, a set of attributes $Q$ depends totally on a set of attributes $P$, denoted $P \rightarrow Q$, if all attribute values from $Q$ can be uniquely determined by values of attributes from $P$. In particular, if there exists a functional dependency between values of $Q$ and $P$, then $Q$ depends totally on $P$. Dependency can be defined in the following way: For $P$, $Q  \subset A$, $Q$  depends on $P$ in a degree $k$ $(0 \leq k \leq 1)$,  denoted $P$ $\rightarrow_{k}$ $Q$, if $k = \gamma_{P}(Q) = |POS_{P}(Q)|/|U|$;  If k = 1 $Q$ depends totally on $P$, if $k < 1 $  $Q$ depends partially (in a degree k) on $P$, and if k = 0 $Q$ does not depend on $P$.

RST performs the reduction of attributes by comparing equivalence relations generated by sets of attributes. Attributes are removed so that the reduced set, termed ``Reduct", provides the same quality of classification as the original. A \emph{reduct} is defined as a subset $R$ of the conditional attribute set $C$ such that $\gamma_{R}(D) = \gamma_{C}(D)$. Thus, a given data set may have many attribute reduct sets. In RST, a \emph{reduct} with minimum cardinality is searched for; in other words an attempt is made to locate a single element of the minimal reduct set. A basic way of achieving this is to generate all possible subsets and retrieve those with a maximum rough set dependency degree. However, this is an expensive solution to the problem and is only practical for very simple data sets. Most of the time, only one reduct is required as, typically, only one subset of features is used to reduce a data set, so all the calculations involved in discovering the rest are pointless. Another shortcoming of finding all possible reducts using rough sets is to inquire about which is the best reduct for the classification process.  The solution to these issues is to apply a \emph{``heuristic attribute selection"} method \cite{zhong2001using}.

Among the most interesting heuristic methods proposed in literature, we mention the \emph{``QuickReduct"} algorithm \cite{shen2007rough} presented by Algorithm \ref{QuickReduct}.

\begin{algorithm}[!ht]
\caption{The QuickReduct Algorithm} \label{QuickReduct}
\begin{algorithmic}[1]
\STATE C: the set of all conditional features;\\
\STATE D: the set of decision features;\\
\STATE R $\leftarrow$ \{\};
\STATE \textbf{do} \\
\STATE \hspace{0.24cm} T $\leftarrow$ R\\
 \STATE \hspace{0.24cm} $\forall$ x $\in$ (C - R);\\
 \STATE \hspace{0.28cm} \textbf{if} $\gamma_{R \cup \{x\}}(D) > \gamma_T(D)$;\\
  \STATE \hspace{0.35cm} T $\leftarrow$ R $\cup$ \{x\};\\
  \STATE \hspace{0.28cm} \textbf{end if} \\
  \STATE \hspace{0.24cm} R $\leftarrow$ T;\\
  \STATE \textbf{until} $\gamma_R(D) == \gamma_C(D)$\\
  \STATE \textbf{return} R
\end{algorithmic}
\end{algorithm}

The \emph{``QuickReduct"} algorithm  attempts to calculate a reduct without exhaustively generating all possible subsets. It starts off with an empty set and adds in turn, one at a time, those attributes that result in the greatest increase in the rough set dependency metric. According to the QuickReduct algorithm, the dependency of each attribute is calculated and the best candidate is chosen. This process continues until the dependency of the reduct equals the consistency of the data set.
For further details about how to compute a  reduct using the   QuickReduct algorithm, we kindly invite the reader to refer to  \cite{shen2007rough}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   METHODOLOGY  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{Followed Methodology}
\label{sec:methodology}

\noindent Before classification techniques are applied, a data preprocessing step has been performed. First, the raw dataset is labelled according a set of \textit{initial corporate security rules}, i.e. every pattern is assigned to a label indication if the corresponding URL request/access would be ALLOWED or DENIED considering these rules. This step is necessary in order to transform the problem into a classification one. However, in order to apply the rules they must be transformed from their initial format into another one that can be applied in our programs (a hash in Perl\footnote{A \textit{hash} in Perl is an object that represents a \textit{hash table}, which is a set of pairs key-value. Sometimes, the value can be another hash itself.}). This is described in Subsection \ref{subsec:ruleparsing}. 

Subsection \ref{subsec:logparsing} details how the patterns of the navigation data log (URL sessions) are also converted to a Perl hash to perform the matting/labelling process. 

At the end of these two steps, the two hashes are compared in order to obtain which entries of the log should be ALLOW or DENY, know as the \textit{labelling} step. This is similar to perform a decision process in a security system. This step results in that there are 38972 pattern belonging to class ALLOW (positive class) and 18530 of class DENY (negative class), so just a 67.78\% of the samples belong to the majority class. This represents a very important problem, since a classifier that is trained considering these proportions is supposed to classify all the samples as ALLOW, getting a theoretically quite good classification accuracy equal or greater than 68\%. However, in section \ref{sec:results} we will see that, despite the fact that some denied patterns are classified as allow, the overall performance of the classifiers are better than the expected.

We observed that more than half of the initial amount of patterns are labelled, and that the ratio is 2:1 in allows to denies. The 2:1 ratio means that the data is unbalanced, and therefore we have performed different approaches from the first and second groups of data balancing techniques, which were introduced in Section \ref{subsec:dataanalysis}:
\begin{itemize}
\item Undersampling: we will randomly remove samples of the majority class until the amount in both classes are similar. In other words, we will reduce the amount of `denied' patterns by a half.
\item Oversampling: we will introduce random samples in the minority class, in order to get a closer number of patterns in both classes. This has to be done due to the impossibility of creating synthetic data when dealing with categorical values (there is not a proper distance measure between two values in a category). Actually, since the number of samples in the majority class is almost twice the minority one, we have just duplicated all of those belonging to the minority class.
\end{itemize}

Finally, in Subsection \ref{subsec:methods} we explain the selection of the methods to apply in order to classify the data. We just have considered the patterns correctly labelled in the preprocessing phase. Thus, a supervised classification process \cite{classification_67} has been conducted on the balanced datasets.
Weka Data Mining Software\footnote{http://www.cs.waikato.ac.nz/ml/weka/} has been used, in order to select the best set of methods in order to deal with these data. These classifiers will be further tested in Section \ref{sec:results}.

% ------------------------------------------------------------------
%
\subsection{Security rules parsing}
\label{subsec:ruleparsing}

\noindent In this work we have considered Drools \cite{drools:site}
as the tool to create, and therefore, manage rules in a business environment. This so called Business Rule Management System (BRMS) has been developed by the JBoss community under an Apache License and it is written in Java. Though this platform consist of many components, here we focus on Drools Expert and the Drools Rule Language (DRL, \cite{drools:doc}). Then, the defined rules for a certain company are inside of a file with a \texttt{.drl} extension, the file that needs to be parsed to obtain the final set of rules. In Figure \ref{fig:drools_hash}, (a), there is the typical rule syntax in DRL. Two main things should be obtained from the parsing method: both left and right sides of the rule, taking into account that the left side is where the company specifies the conditions required to apply the action indicated in the right side. Also, for describing the conditions, Squid syntax is used (see Section \ref{sec:problemDescription}), having thus the following structure: \texttt{squid:Squid(\textit{conditions})}. Finally, from the right side of the rule, the \textit{ALLOW} or \textit{DENY} label to apply on the data that matches with the conditions, will be extracted. The Perl parser that we have implemented applies two regular expressions, one for each side of the rule, and returns a hash with all the rules with the conditions and actions defined. The `before and after' performing the parsing over the \texttt{.drl} file is in Figure \ref{fig:drools_hash}.

\begin{figure}[htb]
\centering
\subfloat[Drools Rule]{
\begin{tabular}{ p{0.05cm} p{0.05cm} p{2.5cm} }
  \texttt{rule~"name"} & & \\
   & \texttt{attributes} & \\
   & \texttt{when} & \\
   & & \texttt{/* Left Side of the Rule */} \\
   & \texttt{then} & \\
   & & \texttt{/* Right Side of the Rule */} \\
  \texttt{end} & & \\
\end{tabular}
}
~
\subfloat[Hash Rule]{
\begin{tabular}{ p{0.05cm} p{0.05cm} p{2.5cm} }
  \texttt{\%rules~=~(} & & \\
   & \texttt{rule~=>\{} & \\
   & & \texttt{field => xxx} \\
   & & \texttt{relation => xxx} \\
   & & \texttt{value => xxx} \\
   & & \texttt{action => [allow, deny]} \\
   & \texttt{\},} & \\
  \texttt{);} & & \\
\end{tabular}
}
\caption{(a) Structure of a rule in Drools Expert. (b) Resulting rule, after the parsing, in a global hash of rules. \label{fig:drools_hash}}
\end{figure}

% ------------------------------------------------------------------
%
\subsection{URL log data parsing}
\label{subsec:logparsing}

\noindent Usually, the instances of a log file have a number of fields, in order to have a registration of the client who asks for a resource, the time of the day when the request is made, and so on. In this case, we have worked with an \textit{access.log} (see Section \ref{sec:problemDescription}) file, converted into a CSV format file so it could be parsed and transformed in another hash of data. All ten fields of the Squid log yield a hash like the one depicted in Figure \ref{fig:data_hash}.

Once the two hashes of data were created, they were compared in such a way that for each rule in the hash of rules, it was determined how many entries in the data log hash are covered by the rule, and so they were applied the label that appears as `action' in the rule.

One of the problems was to extract from a whole URL the part that was
more interesting for our purposes. It is important to point out that
in a log with thousands of entries, an enormous variety of URLs can be
found, since some can belong to advertisements, images, videos, or
even some others does not have a domain name but are given directly by
an IP address. For this reason, we have taken into account that for a
domain name, many subdomains (separated by dots) could be considered,
and their hierarchy grows from the right towards the left. The highest
level of the domain name space is the Top-Level Domain (TLD) at the
right-most part of the domain name, divided itself in country code
TLDs and generic TLDs. Then, a domain and a number of subdomains
follow the TLD (again, from right to left). In this way, the URLs in the
used log are such as \textit{http://subdomain...subdomain.domain.TLD/}
\textit{other\_subdirectories}. However, for the ARFF\footnote{Format
  of Weka files} file to be created, only the domain (without the
subdomains and the TLD) should be considered, because there are too
many different URLs to take into consideration. Hence, applying
another regular expression, the data parser implemented in Perl
obtains all the core domains of the URLs, which makes 976 domains in
total. 

\begin{figure}[htb]
\centering
\begin{tabular}{ p{0.1cm} p{0.1cm} p{6cm} }
  \texttt{\%logdata~=~(} & & \\
   & \texttt{entry~=>\{} & \\
   & & \texttt{http\_reply\_code => xxx} \\
   & & \texttt{http\_method => xxx} \\
   & & \texttt{duration\_miliseconds => xxx} \\
   & & \texttt{content\_type\_MCT => xxx} \\
   & & \texttt{content\_type => xxx} \\
   & & \texttt{server\_or\_cache\_address => xxx} \\
   & & \texttt{time => xxx} \\
   & & \texttt{squid\_hierarchy => xxx} \\
   & & \texttt{bytes => xxx} \\
   & & \texttt{url => xxx} \\
   & & \texttt{client\_address => xxx} \\
   & \texttt{\},} & \\
  \texttt{);} & & \\
\end{tabular}
\caption{Perl hash with an example entry. The actual hash used for this work has a total of 100000 entries, with more than a half labelled as \textit{ALLOW} or \textit{DENY} after the comparing process. \label{fig:data_hash}}
\end{figure}

% ------------------------------------------------------------------
%
\subsection{Classification Methods}
\label{subsec:methods}

\noindent Two reasons have been taken into account when deciding which classifiers would be admitted after what we call `a preselection phase'. On the one hand, our final goal, which is obtaining a set of rules able to classify unknown URL connection requests. To achieve that, we need classifiers based on decision trees, or rules, so that we can study their output in addition to their accuracy. On the other hand, as said in Section \ref{sec:problemDescription}, the features of the data used for this work are mainly categorical, but also numerical. Thus, among the classifiers based on trees or rules, they must also bear with these type of features. Consequently, we made a first selection over all the classifiers in Weka which complied with the requirements. The list, as well as a first set of experiments to see which five were better, can be seen at Table \ref{tabresults_todos}.

In this way, a decision-tree algorithm is a group of conditions organised in a top-down recursive manner in a way that a class is assigned following a path of conditions, from the root of the tree to one of its leaves. Generally speaking, the possible classes to choose are mutually exclusive. Furthermore, these algorithms are also called ``divide-and-conquer'' algorithms. On the other hand, there are the ``separate-and-conquer'' algorithms, which work creating rules one at a time, then the instances covered by the created rule are removed and the next rule is generated from the remaining instances.

A reference to each Weka classifier can be found at \cite{Frank2011}. Below are described the top five techniques, obtained from the best results (See Table \ref{tabresults_todos}) of the experiments done in this stage, along with more specific bibliography. Naïve Bayes method \cite{Bayesian_Classifier_97} has been included as a baseline, normally used in text categorization problems. According to the results, the five selected classifiers are much better than this method.

\begin{table}[htpb]
\centering
 \caption{\label{tabresults_todos} Results of all the tested classification methods on balanced data. The best ones are marked in boldface.}
{\small
\begin{tabular}{|l|l|l|}
\cline{2-3}
\multicolumn{1}{l|}{} & Undersampling & Oversampling \\ 
\hline
Naïve Bayes & 91.12 & 91.77 \\ 
\hline
Conjunctive Rule & 60.14 & 60.02 \\ 
\cline{1-1}
Decision Table & 94.08 & 90.29 \\ 
\cline{1-1}
DTNB & 94.75 & 95.65 \\ 
\cline{1-1}
JRip & 90.08 & 92.47 \\ 
\cline{1-1}
NNge & \textbf{96.49} & \textbf{98.76} \\ 
\cline{1-1}
One R & 93.45 & 93.70 \\ 
\cline{1-1}
PART & \textbf{96.45} & \textbf{97.54} \\ 
\cline{1-1}
Ridor & 87.22 & 89.87 \\ 
\cline{1-1}
Zero R & 51.39 & 51.26 \\ 
\cline{1-1}
AD Tree & 77.73 & 77.68 \\ 
\cline{1-1}
Decision Stump & 60.14 & 60.02 \\ 
\cline{1-1}
J48 & \textbf{97.02} & \textbf{98.00} \\ 
\cline{1-1}
LAD Tree & 79.95 & 79.97 \\ 
\cline{1-1}
Random Forest & \textbf{96.87} & \textbf{98.84} \\ 
\cline{1-1}
Random Tree & 95.14 & 98.35 \\ 
\cline{1-1}
REP Tree & \textbf{96.79} & \textbf{97.67} \\ 
\hline
\end{tabular}
}
\end{table}


\begin{description}
   \item[J48] This classifier generates a pruned or unpruned C4.5 decision tree. Described for the first time in 1993 by \cite{Quinlan1993}, this machine learning method builds a decision tree selecting, for each node, the best attribute for splitting and create the next nodes. An attribute is selected as `the best' by evaluating the difference in entropy (information gain) resulting from choosing that attribute for splitting the data. In this way, the tree continues to grow till there are not attributes anymore for further splitting, meaning that the resulting nodes are instances of single classes. 
   \item[Random Forest] This manner of building a decision tree can be seen as a randomization of the previous C4.5 process. It was stated by \cite{Breiman2001} and consist of, instead of choosing `the best' attribute, the algorithm randomly chooses one between a group of attributes from the top ones. The size of this group is customizable in Weka.
   \item[REP Tree] Is another kind of decision tree, it means Reduced Error Pruning Tree. Originally stated by \cite{Quinlan1987}, this method builds a decision tree using information gain, like C4.5, and then prunes it using reduced-error pruning. That means that the training dataset is divided in two parts: one devoted to make the tree grow and another for pruning. For every subtree (not a class/leaf) in the tree, it is replaced by the best possible leaf in the pruning three and then it is tested with the test dataset if the made prune has improved the results. A deep analysis about this technique and its variants can be found in \cite{Elomaa2001}.
   \item[NNge] Nearest-Neighbor machine learning method of generating rules using non-nested generalised exemplars, i.e., the so called `hyperrectangles' for being multidimensional rectangular regions of attribute space \cite{Martin1995}. The NNge algorithm builds a ruleset from the creation of this hyperrectangles. They are non-nested (overlapping is not permitted), which means that the algorithm checks, when a proposed new hyperrectangle created from a new generalisation, if it has conflicts with any region of the attribute space. This is done in order to avoid that an example is covered by more than one rule (two or more).
   \item[PART] It comes from `partial' decision trees, for it builds its rule set from them \cite{Frank1998}. The way of generating a partial decision tree is a combination of the two aforementioned strategies ``divide-and-conquer'' and ``separate-and-conquer'', gaining then flexibility and speed. When a tree begins to grow, the node with lowest information gain is the chosen one for starting to expand. When a subtree is complete (it has reached its leaves), its substitution by a single leaf is considered. At the end the algorithm obtains a partial decision tree instead of a fully explored one, because the leafs with largest coverage become rules and some subtrees are thus discarded.
 \end{description} 

These methods will be deeply tested on the dataset (balanced and unbalanced) in the following section.

% ------------------------------------------------------------------
%
\subsection{Erasing redundant information}
\label{subsec:duplicateddata}

After having analised the log of connecting patterns, we studied the field \textit{squid\_hierarchy} and saw that had two possible values: \texttt{DIRECT} or \texttt{DEFAULT\_PARENT}. The Squid FAQ reference \cite{squid_logs}, and the Squid wiki \cite{squid_wiki} explain that, as a proxy, the connections are made, firstly to the Squid proxy, and then, if appropriate, the request continues to another server. These connections are registered in Squid the same way, with the same fields, with the exception of the client and server IP addresses. From the point of view of classification, if one of these two entries happens to be in the training file, and the other in the testing file, it would mean that the second would be well classified because of all the attribute values that both have in common. However, this means also that the good percentages that we obtained may not be real, but faked. That is why the second step is about removing entries that we called ``repeated'' (in the explained sense). After the removal, a new set of files was created.

% ------------------------------------------------------------------
%
\subsection{Performing feature selection}
\label{subsec:featselresults}

Having, then, an initial set of 12 extracted features, our aim is to see if we can reduce execution time while preserving good accuracies. Thus, after applying a feature selection method, based on Rough Sets (see Section \ref{sec:featureselection}), we obtained a list of 9 features. 
% Antonio - you should specify the data to which the method has been applied (unbalanced), and justify why have you considered those data (not balanced). This section was thought to be after the first results have been shown...
% Why was thought like that? By whom? As far as I know, for ECTA we put Methodology in one section and in another, the results. Here is the same.

% Antonio - When we proposed this extension we/you should decide which dataset to use. This decision must be commented here, since you have one dataset and another one without duplications, so please clarify here which dataset have you used.
% Moreover, this was a general comment (you must refer to the dataset used in every experiment and explain why have you used it). In the case of the test of the new features I proposed initially to consider all the same datasets (with the partitions). This was a big amount of work, so we decided to use just unbalanced data, because they got better results than balanced datasets. This is what I mean that must be clarified/justified in the text.

The resulting reduced dataset was used to test the same chosen classifiers as before, plus JRip. This is a classifier which consists of a propositional rule learner, the so-called Repeated Incremental Pruning to Produce Error Reduction (RIPPER) algorithm. It was proposed by W. Cohen \cite{cohen1995fast} as an improved version of the Incremental Reduced Error Pruning (IREP) algorithm. The reason why this JRip classifier was added to the list, is because we cannot compare the size of the trees for the Random Forest classifier, as the size of the forest is chosen when running it. Then, we added JRip for making the comparison more complete.

All the experiments were made with the same computer, in the following conditions: Toshiba Laptop with Intel\texttrademark  Core i7-3630QM, CPU@2'40GHz x 8; RAM 3'8 GB; Operating System Linux Ubuntu 14.04 LTS 64 bits; and Weka version 3.6.10, with 3 GB assigned for memory usage. Table \ref{tab_runningtimes} shows the results of the comparison between performance before and after applying feature selection. Though the complexity of the trees generated by the classifiers grow after applying feature selection, even obtaining more rules for PART classifiers, the running time lowers an average of 40\%. This is one of the main objectives when a feature selection technique is applied [REF].
% Antonio - Zeineb, do you have a reference of this to support the statement? ;)

\begin{table}[htpb]
\centering
 \caption{\label{tab_runningtimes} Comparison between rule/tree complexity and running times for the initial data set, which had 12 features, and the resulting one after applying Rough Set Theory Feature Selection, having 9 features.}
{\small
\begin{tabular}{|l|c|c|}
\cline{2-3}
\multicolumn{1}{l|}{} & 12 features & 9 features \\ 
\hline
J48 & Size of the tree 8113, 0.62 seconds & Size of the tree 10191, 0.26 seconds \\ 
\cline{1-1}
Random Forest & 10 trees, 1.59 seconds & 10 trees, 1.32 seconds \\
\cline{1-1}
REP Tree & Size of the tree 8317, 2.24 seconds & Size of the tree 8817, 0.53 seconds \\ 
\cline{1-1}
NNge & 1341 exemplars, 48.11 seconds & 1294 exemplars, 39.57 seconds \\ 
\cline{1-1}
PART & 966 rules, 15.38 & 998 rules, 11.24 seconds \\ 
\cline{1-1}
JRip & 87 rules, 127.28 seconds & 64 rules, 64.23 seconds \\ 
\hline
\end{tabular}
}
\end{table}

Now, if we focus on the results of the accuracies, which are in Table \ref{tab_12featvs9feat}, we see that the classifier accuracies are the same. Therefore, we have demonstrated that applying Rough Set Theory for feature selection significantly improves the computational cost of the system, without loss of classifier accuracy.

\begin{table}[htpb]
\centering
 \caption{\label{tab_12featvs9feat} Comparison between obtained accuracies for the initial data set, which had 12 features, and the resulting one after applying Rough Set Theory Feature Selection, having 9 features.}
{\small
\begin{tabular}{|l|c|c|}
\cline{2-3}
\multicolumn{1}{l|}{} & 12 features & 9 features \\ 
\hline
Naïve Bayes & 92.30 & 92.19 \\ 
\cline{1-1}
J48 & 97.37 & 97.40 \\ 
\cline{1-1}
Random Forest & 97.60 & 97.57 \\
\cline{1-1}
REP Tree & 97.33 & 97.34 \\ 
\cline{1-1}
NNge & 97.18 & 97.13 \\ 
\cline{1-1}
PART & 97.41 & 97.27 \\ 
\cline{1-1}
JRip & 92.95 & 91.66 \\ 
\hline
\end{tabular}
}
\end{table}

% Antonio - I think that would be interesting to show and analyse (justify them) the selected features

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   RESULTS  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{\uppercase{Results}}
\label{sec:results}

\noindent Several experiments have been conducted, once a subset of classification methods has been chosen in previous section.
To this end, some training and test datasets have been created from
the set of labelled patterns. It contains 57502 samples, with 38972
belonging to class ALLOW and 18530 to class DENY.

In order to better test the methods, two different divisions (training-test) have been done, namely 90\%-10\% and 80\%-20\%. Moreover, two additional splits have been considered in every case, using both a random and a sequential approach for selecting samples from the original file. Thus, in the latter, consecutive patterns have been included in the training file up to the desired percentage. The rest have composed the test file. In the first approach, a random selection is performed.

The aim of the sequential division is to compare if the online activity of the employees considering URL sessions could be somehow `predicted', just using data from previous minutes or hours.

With respect to the data, the initial file was unbalanced, as it can be seen in the number of patterns per class. Hence, as stated in Section \ref{sec:problemDescription}, two data balancing methods have been applied to all the files, to get similar numbers in both classes: undersampling (random removal of ALLOW patterns) and oversampling (duplication of DENY patterns).

Results for unbalanced data are presented in Table \ref{tabresults_nobalan}.
Three different tests have been done for the random pattern distribution approach, so the mean and standard deviation are shown in the corresponding columns.

\begin{table*}[htpb]
\centering
 \caption{\label{tabresults_nobalan} Percentage of correctly classified patterns for unbalanced data}
% Antonio - try to use the same term always 'unbalanced' or 'non-balanced'.
%           Moreover, the captions should be more explicit, I mean, specify if these results are obtained for the initial data or for the new set of 9 features.
% Please, revise the rest o captions in this sense.
{\small
\begin{tabular}{|l|l|l|l|l|}
\cline{2-5}
\multicolumn{1}{l|}{} & \multicolumn{2}{c|}{80\% Training - 20\% Test} & \multicolumn{2}{c|}{90\% Training - 10\% Test} \\ 
\cline{2-5}
\multicolumn{1}{l|}{} & Random (mean) & Sequential & Random (mean) & Sequential \\ 
\hline
J48 & 97.56 $\pm$ 0.20 & 88.48 & 97.70 $\pm$ 0.15 & 82.28 \\ 
\cline{1-1}
Random Forest & 97.68 $\pm$ 0.20 & 89.77 & 97.63 $\pm$ 0.13 & 82.59 \\ 
\cline{1-1}
REP Tree & 97.47 $\pm$ 0.11 & 88.34 & 97.57 $\pm$ 0.01 & 83.20 \\ 
\cline{1-1}
NNge & 97.23 $\pm$ 0.10 & 84.41 & 97.38 $\pm$ 0.36 & 80.34 \\ 
\cline{1-1}
PART & 97.06 $\pm$ 0.19 & 89.11 & 97.40 $\pm$ 0.16 & 84.17 \\ 
\hline
\end{tabular}
}
\end{table*}
 
As it can be seen, all the five methods achieved a high performance classifying in the right way the test dataset. Also, these results are not like this by chance, as shown by a low standard deviation. Although it was expected that the results from the 90\%-10\% division were slightly better, in the future a more aggressive division will be executed so the methods can be really proved with much less training data.

What matters to the results of the experiments made with the sequential data, they are worse than the obtained from the random data, but still they are good ($>$ 85\%). This is due to the occurrence of new patterns from a certain time (maybe there are some requests that are made just at one specific time in a day, or in settled days), and then there is no sufficient similarity between the training data and the classifying of the test data set may fail. The loss of 5 to 6 points in the results of the 90\%-10\% division is the first unexpected or unlogical result of the experiments, but they also reinforce the previous theory.

The technique that lightly stands out over the others is
\textit{Random Forest}, being the best in almost every case, even in
the experiments with the most complex sequential divisions. However,
if we focus on the standard deviation, \textit{REP Tree} is the chosen
one, as its results present robustness. 

For its part, results obtained from unbalanced data are shown in Table \ref{tabresults_balan}. Again the corresponding to the random partitions come from the mean of three blocks of experiments, and so are specified the standard deviations. The Table illustrates two segments of results, obtained from the undersampled data and from the oversampled data. For each one, the 90\%-10\% and 80\%-20\% divisions were also made.

\begin{table*}[htpb]
\centering
 \caption{\label{tabresults_balan} Percentage of correctly classified patterns for balanced data (under- and oversampling)}
{\small
\begin{tabular}{|l|l|l|l|l|l|l|l|l|}
\cline{2-9}
\multicolumn{1}{l|}{} & \multicolumn{4}{c|}{80\% Training - 20\% Test} & \multicolumn{4}{c|}{90\% Training - 10\% Test} \\ 
\cline{2-9}
\multicolumn{1}{l|}{} & \multicolumn{2}{c|}{Undersampling} & \multicolumn{2}{c|}{Oversampling} & \multicolumn{2}{c|}{Undersampling} & \multicolumn{2}{c|}{Oversampling} \\ 
\cline{2-9}
\multicolumn{1}{l|}{} & Rand (mean) & Sequential & Rand (mean) & Sequential & Rand (mean) & Sequential & Rand (mean) & Sequential \\ 
\hline
J48 & 97.05 $\pm$ 0.25 & 84.29 & 97.40 $\pm$ 0.03 & 85.66 & 96.85 $\pm$ 0.35 & 76.44 & 97.37 $\pm$ 0.06 & 74.24 \\ 
\cline{1-1}
Random Forest & 96.61 $\pm$ 0.17 & 88.59 & 97.16 $\pm$ 0.19 & 89.03 & 96.99 $\pm$ 0.13 & 79.98 & 97.25 $\pm$ 0.33 & 81.33 \\ 
\cline{1-1}
REP Tree & 96.52 $\pm$ 0.13 & 85.54 & 97.13 $\pm$ 0.25 & 85.41 & 96.55 $\pm$ 0.10 & 77.65 & 97.14 $\pm$ 0.09 & 76.81 \\ 
\cline{1-1}
NNge & 96.56 $\pm$ 0.42 & 85.28 & 96.90 $\pm$ 0.28 & 83.46 & 96.33 $\pm$ 0.05 & 81.93 & 96.91 $\pm$ 0.06 & 78.73 \\ 
\cline{1-1}
PART & 96.19 $\pm$ 0.14 & 85.16 & 96.82 $\pm$ 0.09 & 84.50 & 96.09 $\pm$ 0.10 & 79.70 & 96.68 $\pm$ 0.11 & 78.16 \\ 
\hline
\end{tabular}
}
\end{table*}

\begin{description}
  \item[Applying Undersampling] In comparison with those results from Table \ref{tabresults_nobalan}, these go down one point (in the case of randomly made divisions) to six points (sequential divisions). The reason why this happens is that when randomly removing ALLOW patterns, we are really losing information, i. e. key patterns that could be decisive in a good classification of a certain set of test patterns. 
  \item[Applying Oversampling] Here we have duplicated the DENY patterns so their number could be up to that of the ALLOW patterns. However, it does not work as well as in other approaches which uses numerical computations for creating the new patterns to include in the minority class. Consequently, the results have been decreased.
\end{description}

In both cases it is noticeable that taking the data in a sequential way, instead of randomly, lower the results. It is clear that due to the fact that performing undersampling some patterns are lost while in the case of oversampling they all remain, \textit{undersampling results} are better. Then, in this case the algorithm with best performance is \textit{J48}, though \textit{Random Forest} follows its results very closely in random datasets processing, and \textit{REP Tree}, which is better than the rest when working with sequential data. Nevertheless, generally speaking and given the aforementioned reasons, performing data balancing methods yields worse results.

Furthermore, we have found that for the data sets taken consecutively, the methods always classify worse the DENY labels, as they label them as ALLOW patterns. This is worth further study because it is the worst situation. It would be preferable to have a false positive in a DENY pattern, rather than a false negative and permit a request that is forbidden in the ISP.

%%%%%%%%%%%%%%% HERE RESULTS AFTER REMOVAL OF DUPLICATED REQUESTS

After performing the removal of duplicated requests, a whole new ranking of classification performance was created, in order to see if the new situation has some influence on the results. However, Table  \ref{tab:gobalrank_repurls} shows that the best results are from the same classifiers, with slightly difference when performing oversampling, where \textit{Random Tree} was better than \textit{REP Tree} but less than one point.

\begin{table}[htpb]
\centering
{\small
\begin{tabular}{|l|c|c|c|}
\cline{2-4}
\multicolumn{1}{l|}{} & Unbalanced & Undersampling & Oversampling \\
\hline
Naïve Bayes & 92.30 & 90.77 $\pm$ 0.06 & 91.77 \\
\cline{1-1}
Conjunctive Rule & 73.31 & 59.53 $\pm$ 0.15 & 60.02 \\
\cline{1-1}
Decision Table & 95.21 & 93.73 $\pm$ 0.18 & 90.29 \\
\cline{1-1}
DTNB & 95.55 & 94.75 $\pm$ 0.07 & 95.65 \\
\cline{1-1}
JRip & 92.95 & 89.64 $\pm$ 0.32 & 92.47 \\
\cline{1-1}
NNge & \textbf{97.18} & \textbf{96.33 $\pm$ 0.14} & \textbf{98.76} \\
\cline{1-1}
One R & 94.86 & 93.53 $\pm$ 0.04 & 93.70 \\
\cline{1-1}
PART & \textbf{97.41} & \textbf{96.32 $\pm$ 0.10} & \textbf{97.54} \\
\cline{1-1}
Ridor & 89.21 & 86.19 $\pm$ 0.79 & 89.87 \\
\cline{1-1}
Zero R & 68.14 & 51.68 $\pm$ 0.17 & 51.26 \\
\cline{1-1}
AD Tree & 85.23 & 78.01 $\pm$ 0.10 & 77.68 \\
\cline{1-1}
Decision Stump & 73.31 & 59.53 $\pm$ 0.15 & 60.02 \\
\cline{1-1}
J48 & \textbf{97.37} & \textbf{96.65 $\pm$ 0.04} & \textbf{98.00} \\
\cline{1-1}
LAD Tree & 86.87 & 80.24 $\pm$ 0.04 & 79.97 \\
\cline{1-1}
Random Forest & \textbf{97.57} & \textbf{96.75 $\pm$ 0.05} & \textbf{98.84} \\
\cline{1-1}
Random Tree & 96.11 & 95.45 $\pm$ 0.66 & \textbf{98.35} \\
\cline{1-1}
REP Tree & \textbf{97.32} & \textbf{96.38 $\pm$ 0.07} & 97.67 \\
\hline
\end{tabular}
}
\caption[Global classification methods ranking after the removal of entries that could lead to missclassification.]{\label{tab:gobalrank_repurls} Results of all tested classification methods on unbalanced and balanced data, after the repeated requests have been removed. The best ones are marked in boldface.}
\end{table}

The process is again the same, and the results are displayed in Tables \ref{tab:repurl_unb_traintest} (a) and (b). We can see that the results are slightly worse than the ones obtained in step 1, but they are still good, and definitely better than Naïve Bayes, our result reference. It is not a surprise that, again, results for files with the patterns taken consecutively lower significantly. As previously, this is because the possible loss of information. Best results are obtained by both \textit{Random Forest} and \textit{REP Tree} classifiers, with a 96\% of accuracy.

\begin{table}[htb]
\centering
\subfloat[80\% for training, \%20 for testing, and 90\% for training, 10\% for testing]{
\small
\begin{tabular}{|l|c|c|c|c|}
\cline{2-5}
\multicolumn{1}{l|}{} & \multicolumn{2}{c|}{80\% Training - 20\% Test} & \multicolumn{2}{c|}{90\% Training - 10\% Test} \\ 
\cline{2-5}
\multicolumn{1}{l|}{} & Random (mean) & Sequential & Random (mean) & Sequential \\ 
\hline
Naïve Bayes & 93.01 $\pm$ 0.32 & 82.61 & 93.09 $\pm$ 0.91 & 83.04 \\ 
\cline{1-1}
Random Forest & \textbf{96.97} $\pm$ \textbf{0.47} & \textbf{91.03} & \textbf{96.79} $\pm$ \textbf{0.97} & 80.60 \\ 
\cline{1-1}
J48 & 96.90 $\pm$ 0.26 & 87.78 & 96.50 $\pm$ 1.00 & 84.49 \\ 
\cline{1-1}
NNge & 96.21 $\pm$ 0.28 & 81.17 & 96.11 $\pm$ 1.13 & 81.92 \\ 
\cline{1-1}
REP Tree & \textbf{96.97} $\pm$ \textbf{0.40} & 87.75 & 96.62 $\pm$ 0.87 & \textbf{85.57} \\ 
\cline{1-1}
PART & 96.84 $\pm$ 0.18 & 86.68 & 96.55 $\pm$ 0.87 & 83.61 \\ 
\hline
\end{tabular}
}

\subfloat[60\% for training, 40\% for testing]{
\small
\begin{tabular}{|l|c|c|}
\cline{2-3}
\multicolumn{1}{l|}{} & \multicolumn{2}{c|}{60\% Training - 40\% Test} \\ 
\cline{2-3}
\multicolumn{1}{l|}{} & Random (mean) & Sequential\\ 
\hline
Naïve Bayes & 92.76 $\pm$ 1.34 & 77.74 \\ 
\cline{1-1}
Random Forest & \textbf{96.45} $\pm$ \textbf{0.67} & 87.50 \\ 
\cline{1-1}
J48 & 96.44 $\pm$ 0.19 & 87.42 \\ 
\cline{1-1}
NNge & 95.98 $\pm$ 0.34 & 84.73 \\ 
\cline{1-1}
REP Tree & 96.41 $\pm$ 0.27 & \textbf{88.91} \\ 
\cline{1-1}
PART & 96.05 $\pm$ 0.45 & 81.29 \\ 
\hline
\end{tabular}
}
\caption[Correctly classified patterns for unbalanced data after the removal of entries that could lead to missclassification.]{Percentage of correctly classified patterns for unbalanced data, after the removal of entries that could lead to missclassification.. (a) 80\% for training, \%20 for testing, and 90\% for training, 10\% for testing. (b) 60\% for training, 40\% for testing. Best results are marked in boldface. \label{tab:repurl_unb_traintest}}
\end{table}

%%%%% From here, results after feature selection

More results in \ref{tab_featselect} show that our results are awesome and your argument is invalid.
% Antonio - I think that we should remove the joke.

\begin{table*}[htpb]
\centering
 \caption{\label{tab_featselect} Percentage of correctly classified patterns for non-balanced data}
{\small
\begin{tabular}{|l|c|c|c|c|}
\cline{2-5}
\multicolumn{1}{l|}{} & \multicolumn{2}{c|}{80\% Training - 20\% Test} & \multicolumn{2}{c|}{90\% Training - 10\% Test} \\ 
\cline{2-5}
\multicolumn{1}{l|}{} & Random (mean) & Sequential & Random (mean) & Sequential \\ 
\hline
J48 & 96.97 $\pm$ 0.17 & 87.83 & 97.09 $\pm$ 1.03 & 84.51 \\ 
\cline{1-1}
Random Forest & 97.68 $\pm$ 0.67 & 88.06 & 97.80 $\pm$ 1.17 & 83.71 \\ 
\cline{1-1}
REP Tree & 97.14 $\pm$ 0.10 & 87.79 & 97.18 $\pm$ 0.71 & 85.73 \\ 
\cline{1-1}
NNge & 96.27 $\pm$ 0.14 & 82.18 & 97.27 $\pm$ 1.57 & 80.94 \\ 
\cline{1-1}
PART & 97.17 $\pm$ 0.05 & 87.88 & 97.08 $\pm$ 1.14 & 85.11 \\ 
\hline
\end{tabular}
}
\end{table*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   RULES  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{\uppercase{Discussion on the Obtained Rules}}
\label{sec:rulesdiscussion}

One of the main objectives of this paper is to find a method (classifier) that can build rules not dependent on the URL, in order to get a behaviour quite different from the classical black and white lists. Thus, it could made a decision about new connection requests based in other (more general) features.

In the performed experiments the majority of the obtained rules/trees are based on the URL in order to discriminate between the two classes, however we also found several ones which consider variables/features different of this to make the decision. For instance:\\

\begin{small}
\begin{verbatim}
IF server_or_cache_address = "173.194.34.225"
AND http_method = "GET"
AND duration_milliseconds > 52
THEN ALLOW

IF server_or_cache_address = "173.194.78.103" 
THEN ALLOW

IF content_type = 
 "application/vnd.google.safebrowsing-update" 
THEN DENY

IF server_or_cache_address = "173.194.78.94" 
AND content_type_MCT = "text"
AND content_type = "text/html"
AND http_reply_code = "200"
AND bytes > 772
THEN ALLOW
\end{verbatim}
\end{small}

These are the interesting rules for our purposes, since they are somehow independent of the URL to which the client requests to access. Thus, it would be potentially possible to allow or deny the access to unknown URLs just taking into account some parameters of the session.
In the case that the features considered in the rule can be known in advance, such as \texttt{http\_method}, or \texttt{server\_or\_cache\_address}, for instance, the decision could be made in real-time, and thus, a granted URL (Whitelisted) could be DENIED or the other way round.

The tree-based methods also yield several useful branches in this sense, but they have not been plotted here because of the difficulty for showing/visualizing them properly.

Focusing on the showed rules, it can be noticed that almost all of them also depend on very determining features/values, such as \texttt{server\_or\_cache\_address}, or even on the \texttt{client\_address} (not showed here).
These features create several non-useful rules, mainly in the case of the client IP address. Because we can not settle that a specific IP could not access to some URLs.

Thus, we have conducted additional experiments in this line, just removing these three features (\texttt{url}, \texttt{server\_or\_cache\_address}, \texttt{client\_address}) from the dataset and training again the classifiers.
These have been performed over the unbalanced data, considering a 10-fold cross validation test.

The results of classification accuracies are shown in Table \ref{tab_rules_study_classification}.

\begin{table}[htpb]
\centering
 \caption{\label{tab_rules_study_classification} Percentage of correctly classified patterns for the unbalanced dataset without \texttt{url} feature and without determining features (namely \texttt{url}, \texttt{server\_or\_cache\_address}, and \texttt{client\_address})}
{\small
\begin{tabular}{|l|c|c|}
\cline{2-3}
\multicolumn{1}{l|}{} & Without URL feature & Without URL-IPs features\\ 
\hline
J48 & 93.62 & 90.53 \\ 
\cline{1-1}
Random Forest & 94.42 & 91.75 \\
\cline{1-1}
REP Tree & 92.58 & 89.61 \\ 
\cline{1-1}
PART & 93.40 & 88.25 \\ 
\cline{1-1}
JRip & 87.45 & 85.60 \\ 
\hline
\end{tabular}
}
\end{table}

% Antonio - I have to change the name of 'determining features', I don't like it very much, but cannot think in a better option... 

As expected, and as it can be seen in the table, the percentages of accuracies have been decreased. This is more dramatic in the case where three features have been discarded.
However, the results are still quite good, having in mind that the remaining features are more general than these.

Anyway, what is really important in these experiments is to analyse the set of rules that the classifiers\footnote{We can deploy the trees as rules also.} have obtained as models.

Thus, having a look at these rules, in the case without \texttt{url} (i.e. 11 features), the rules are pretty similar to those presented before. Thus, we can find, among the most important rules (in the sense that the classification accuracy depends in a big part on them): 

\begin{small}
\begin{verbatim}
IF bytes >= 1075
AND time >= 29633000
AND time <= 30031000
AND client_address = "10.159.52.182"
AND content_type_MCT = "image"
AND content_type = image/jpeg
THEN DENY

IF server_or_cache_address = "173.194.66.121" 
AND client_address = "192.168.4.4"
AND time <= 33603000
THEN ALLOW

IF client_address = "10.159.188.11"
AND bytes <= 2166 
AND content_type_MCT = "text"
THEN ALLOW
\end{verbatim}
\end{small}

Which depends on the so-called `determining features'. Actually, almost all the important rules depend on them.

Due to this reason we have performed another experiment omitting these variables, in addition to the \texttt{url}, as it has been commented before.

The generated rules for some of the classifiers (tree and rule-based) in this experiment are closer to what we want to obtain. Some examples of relevant rules are:

%[J48], [REP Tree], [PART] and [PART]
\begin{small}
\begin{verbatim}

IF http_reply_code = "200"
AND content_type = "application/json"
AND time <= 33635000
AND bytes <= 3921
THEN ALLOW

IF content_type = "text/plain"
AND duration_milliseconds >= 7233.5 
THEN DENY

IF content_type = "application/octet-stream"
AND bytes <= 803
THEN ALLOW

IF bytes <= 1220
AND time <= 33841000
AND http_reply_code = "404"
AND squid_hierarchy = DEFAULT_PARENT
AND duration_milliseconds <= 233
AND bytes <= 722
THEN ALLOW
\end{verbatim}
\end{small}

As it can be seen these are more general rules which could be much more useful to classify new URL accesses requests (non-previously known) in a company.
These rules could be taken as a reference to build a decision system, but there still some considerations to have into account, since all the rules have been created using very specific data. Thus, there are several rules that cannot be used as they are in other companies, and should be supervised somehow (maybe by a human expert).

Moreover, some of these features depend on the session itself, i.e. they will be computed after the session is over, but the idea in that case would be `to refine' somehow the existing set of URLs in the White List.
Thus, when a client requests access to a Whitelisted URL, this will be allow, but after the session is over, and depending on the obtained values, and on one of these classifiers, the URL could be labelled as DENIED for further requests.
This could be a useful decision-aid tool for the CSO in a company, for instance.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  CONCLUSIONS  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{\uppercase{Conclusions and Future Work}}
\label{sec:conclusions}

\noindent In this paper a set of classification methods have been applied
in order to perform a decision process inside a company, according to
some predefined corporate security policies. 
This decision is focused on allowing or denying URL access requests, but just considering previous decisions on similar requests, not having specific rules in a White/Black List, defined for those URLs. Thus, the proposed method could allow or deny an access to a URL based in additional terms rather than just the specific URL string. This could be very useful since new URLs could be automatically 'Whitelisted' or 'Blacklisted', just depending on some of the connection parameters, such as the \texttt{content\_type} of the access or the \texttt{IP} of the client which makes the request.

To this aim, we have started from a big dataset (100000 patterns) about employees' URL sessions information, and considering a set of URL access permissions, we have composed a labelled dataset (57000 patterns). Over that set of data, we have tested several classification methods, after some data balancing techniques have been applied. Then, the best five have been deeply proved over several training and test divisions, and with two methods: using sequential patterns (consecutive URL accesses), and taking them in a randomly way.

The results show that classification accuracies are between 95\% and 97\%, even when using the unbalanced datasets. However, they have been diminished because of the possible loss of data that comes from performing an undersampling (removing patterns) method; or taking the training and the data sets in a sequential way from the main log file, due to the fact that certain URL requests can be made only at a certain time.

In this way, we can conclude that the approach has been successful and it would be a useful tool in an enterprise.

Future lines of work include conducting a deeper set of experiments trying to test the generalisation power of the method, maybe considering bigger data divisions, bigger data sets (from a whole day or working day), or adding some kind of `noise' to the dataset.
So that, considering the good classification results obtained in this work, the next step could be the application of these methods in the real system from which data was gathered, counting with the opinion of expert CSOs, in order to know the real value of the proposal.
The study of other classification methods could be another research branch, along with the implementation of a Genetic Programming approach, which could deal with the imbalance problem using a modification of the cost associated to misclassifying, could be done (as the authors did in \cite{cost_adjustment_07}).

Finally, we also point to extract additional information from the URL string, than could be transformed into additional features that could be more discriminative than the current set. Moreover, a data process involving summarizing data about sessions (such as number of requests per client, or average time connection) will be also considered.



\section*{Acknowledgements}
This paper has been funded in part by European project MUSES (FP7-318508), along with Spanish National project TIN2011-28627-C04-02 (ANYSELF), project P08-TIC-03903 (EVORQ) awarded by the Andalusian Regional Government, and projects 83 (CANUBE), and GENIL PYR-2014-17, both awarded by the CEI-BioTIC UGR.

\bibliographystyle{splncs03}
\bibliography{data_mining_urls}


\end{document}
