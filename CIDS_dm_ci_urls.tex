\documentclass{llncs}

\usepackage[latin1]{inputenc}
\usepackage{epsfig}
\usepackage{graphicx}        % standard LaTeX graphics tool
%\usepackage{subfigure}
\usepackage{url}
\usepackage{subfig}
\usepackage{calc}
%\usepackage{amssymb}
%\usepackage{amstext}
%\usepackage{amsmath}
%\usepackage{amsthm}
\usepackage{multicol}
\usepackage{pslatex}
\usepackage{caption}


\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   TITLE   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Application of Data Mining, Machine Learning and Computational Intelligence techniques to Corporate Security}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   AUTHORS   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\author{P. de las Cuevas, Z. Chelly, A. M. Mora, J.J. Merelo, A.I. Esparcia-Alcázar}
\authorrunning{P. de las Cuevas et al.}

\institute{Department of Computer Architecture and Computer Technology, University of Granada, Spain. \\
Laboratoire de Recherche Opérationelle de Décision et de Contrôle de Processus, Institut Supérieur de Gestion, Tunisia.\\
S2 Grupo, Spain\\
 {\tt \{paloma,amorag,jmerelo\}@geneura.ugr.es, zeinebchelly@yahoo.fr, aesparcia@s2grupo.es}
}

\maketitle

%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   ABSTRACT   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\begin{abstract} 
Corporate security is usually one of the matters in which enterprises invest more resources, because the loss of information translates into money loss. Companies may receive external attacks, which sometimes are deliberately made by hackers. Moreover, information leakage can be caused by employees that were fired. Finally, an important part of the security breaches is related to the lack of awareness that the employees have with regard to the use of the Internet. In this work we have focused in the latter problem, creating and improving a system able to detect anomalous, and potentially unsecure, situations that could be dangerous for a company. This system was initially conceived as a step beyond the use of self-updating blacklists and whitelists. These lists contain URLs that are forbidden to access, or dangerous (blacklists), or URLs to which the access is permitted or allowed (whitelists). There are websites with daily updated information about new malicious URLs, so that one can update the set of denied/allowed connections. However, as the number of websites constantly grows, and the company cannot know in advance if they are malicious or not, the system we proposed has the purpose of learning from the already known black/white lists, and classifying them as ``should be allowed'' or ``should be denied'' a new, unknown, URL connection. This chapter describes the proposed system, as well as its results, and the improvements made, by means of applying Rough Set Theory for feature selection.
\end{abstract}


%\textbf{Keywords.} Video Games, Super Mario Bros, Genetic Algorithms,
%Artificial Intelligence, Non Player Character, Finite State Machine

%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   INTRODUCTION   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{Introduction}
\label{sec:introduction}

\noindent Security is a large term that refers to a diversity of steps taken by individuals, and companies, in order to protect computers or computer networks that are connected to the Internet. The Internet was initially conceived as an open network facilitating the free exchange of information. However, data which is sent/received over the Internet travel through a dynamic chain of computers and network links. As a consequence, the risk of intercepting and changing the data is high. In fact, it would be virtually impossible to secure every computer connected to the Internet around the world. So, there will likely always be weak links in the chain of data exchange \cite{cheswick2003firewalls}. Yet, companies have to find out a way to safely interact with customers, clients, and anyone who uses the Internet while protecting internal confidential information. Companies have, also, to alert the employees from the Internet misuse while doing their job.
 
Most of the time, employees have a misguided sense of security and believe that it is an IT problem, purely technical issue, and they naively believe that an incident may never happen to them. Actually, the employees' Internet misuse is one of the main causes of security breaches, and so it has become a security challenge. The reality is that every department must be involved in readiness planning and establishing security policies and procedures to minimize their risks. Such strategies are mainly handled by means of \textit{Corporate Security Policies} (CSPs) which basically are a set of security rules aiming at protecting company assets by defining permissions to be considered for every different action to be performed inside the security system.
 
The basic idea behind these CSPs is that they usually include policies to either allow or deny employees' access to non-confident or non-certified web sites, which are referenced by their URLs in the current work. Moreover, several web pages might be also controlled for productivity or suitability reasons, given the fact that the employees who connect to these might have working purposes or not. Actually, some of the CSPs usually define sets of allowed or denied web pages or web sites that could be accessed by the company employees. These sets are usually included in two main lists; a white list (referring to ``permitted'') and a black list (referring to ``non-permitte''). Both lists, the white and the black, act as a good and useful control tools for those URLs included in them, as well as for the complementary. For instance, the URLs which are not included in a white list will automatically have a denial of access.

The aim of this paper is going beyond this traditional and simple decision making process. By using black and/or white lists, we either allow or deny users' requests/connection based, only, on the URLs provided in the lists. Thus, our aim is to define a tool for automatically making an allowance or a denial decision with respect to URLs that are, actually, not included in the aforementioned lists. This decision would be based on that one made for similar URL accesses (those with similar features), but instead of using, only, the URL strings included in the lists, we will consider other parameters, indeed, of the request/connection.
For this reason, the problem has been mapped to a \textit{classification} problem in which we start from a set of unlabelled patterns that model the connection properties from a huge amount of \textit{actual}\footnote{Taken from a log file given by a volunteer Spanish company.} URL accesses, known as sessions. After that, we assign a label to many of them, considering a set of \textit{actual}\footnote{The set of rules has been written by the same company, with respect to its employees.} security rules (CSPs) defined by the Chief Security Officer (CSO) in the company.

In order to extract conclusions from the resulting studied dataset and to properly apply a classification algorithm, a pre-processing step is needed. In fact, to obtain an accurately trained classifier, there is need for extracting as much information as possible from the connections that the employees normally make throughout the workday. This translates into high computational requirements, which is why we use techniques for data reduction. More precisely, we aim to apply a feature selection technique to extract the most important features from the data at hand. Among the well known feature selection techniques proposed in literature, we propose to use Rough Set Theory (RST) \cite{pawlak2008rough}. RST has been experimentally evaluated with other leading feature selection techniques, such as Relif-F and entropy-based approaches in \cite{jensen2007fuzzy}, and has been shown to outperform these in terms of resulting classification performance.
 
After pre-processing and based on the reduced dataset, we tend to apply several classification algorithms, testing them and selecting the most appropriate one. The selected classifier would be capable of dealing with our data while producing high accuracies and being lightweight in terms of running time. Moreover, as we want to further test the reliability of the results, in this work we propose different experimental setups based on different data partitions. These partitions are formed either by preserving the order of the data or by taking the patterns in a random way. Finally, given that the used data presents unbalance, we aim to apply balancing techniques \cite{imbalance_techniques_02}.

In this paper, we demonstrate that high accuracies can be obtained with a proper pre-processing of the data, reaching between 95\% and 97\% of correctly classified patterns. Indeed, among the obtained results, we demonstrated that it is possible to obtain useful rules, which are not based only on the URL string, for classifying new unknown URLs as allowed or as denied.

%Change "Section _" for its reference. (Paloma)

The rest of the paper is structured as follows. Next section describes the state of the art related to Data MIning (DM), Machine Learning (ML), and Computational Intelligence (CI) techniques applied to enterprise security. Also, related works about URL filtering will be reviewed. Data description is detailed in Section 3. Then, Section 4 describes the basic concepts of Rough Set Theory for feature selection which we have used for data pre-processing. Section 5 gives an overview of the followed methodology, as well as the improvements done after our first results obtained in \cite{ECTA}. Section 6 discusses the obtained rules which are different for every used classifier. Finally, conclusions and future trends are given in Section 7. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  STATE OF THE ART  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{State of the Art}
\label{sec:stateofart}

\noindent Our work tries to obtain a URL classification tool for enhancing the security in the client side, as at the end we want to get if a certain URL is secure or not, having as reference a set of rules (derived from a CSP) that allow or deny a set of known \textit{http} requests. For this, Data Mining (DM) and Machine Learning (ML) techniques have been applied. This section gives an overview in a number of solutions given to protect the user, or the company, against unsecure situations.

Due to the nature of the data (URL accesses performed by humans), the used set of data is highly unbalanced \cite{imbalanced_data_05}. In order to deal with this problem there exist several methods in the
literature, but all of them are mainly grouped in three techniques
\cite{imbalance_techniques_02}: 

\begin{itemize}
\item \textit{Undersampling the over-sized classes}: i.e. reduce the considered number of patterns for the classes with the majority.
\item \textit{Oversampling the small classes}: i.e. introduce additional (normally synthetic) patterns in the classes with the minority.
\item \textit{Modifying the cost associated to misclassifying the positive and the negative class} to compensate for the imbalance ratio of the two classes. For example, if the imbalance ratio is 1:10 in favour of the negative class, the penalty of misclassifying a positive example should be 10 times greater.
\end{itemize}

The first option has been applied in some works, following a random undersampling approach \cite{random_undersampling_08}, but it has the problem of the loss of valuable information. 

The second has been so far the most widely used, following different approaches, such as SMOTE (Synthetic Minority Oversampling Technique) \cite{smote_02}, a method proposed by Chawla et al. for creating `artificial' samples for the minority class, in order to balance the amount of them with respect. However this technique is based in numerical computations, which consider different distance measures, in order to generate useful patterns  (i.e. realistic or similar to the existing ones).

The third option implies using a method in which a cost can be associated to the classifier accuracy at every step. This was done for instance by Alfaro-Cid et al. in \cite{cost_adjustment_07}, where they used a Genetic Programming (GP) approach in which the fitness function was modified in order to consider a penalty when the classifier makes a false negative (an element from the minority class was classified as belonging to the majority class).
However almost all the approaches deal with numerical (real, integer) data.

One interesting point about URL classification is that the study of the distance between URLs may be based in the distance between two strings, but Blanco et al. \cite{Blanco2011} argues that the lexical distance between two URLs is not enough to classify them. In addition, the heuristic study of URLs for security purposes in the user side is not a novel practice. Also, the use of Blacklists (in this work, the \textit{denied} URLs) and Whitelists (\textit{allowed} URLs) are very extended practices. For instance, phishing is a problem of security that Sheng et al.  and Khonki et al. \cite{Khonji2011} tried to solve. The first work uses Blacklists as reference to avoid phishing attacks made by e-mail; the second one aims for an heuristic analysis of the URLs domain names and its ranks, in a way that a phished URL can be detected.

Also, doing some web searching we have found that a lot of companies stands for the use of one between Blacklist and Whitelist \footnote{http://kevtownsend.wordpress.com/2011/08/24/whitelisting-vs-blacklisting/}. While whitelisting is the more restrictive solution and therefore the more secure, we think that the best solution is to use both, and for this reason the set of rules that we used covers a succession of either allowed and denied web sites.

What refers to the used techniques, DM, as well as ML, has been used since long ago in many scientific fields,
and given that research in computer security was growing since the
eighties \cite{computer_security_80}, it was in the nineties
when these techniques began to be applied to security issues
\cite{Clifton1996}. 

On the one hand, DM helped to develop new solutions to computer forensics \cite{DeVel2001}, being the researchers able to extract information from large files with events gathered from infected computers. Another important advance took place after the 9/11 events, when \textit{clustering techniques} and \textit{social network analysis} started to be performed in order to detect pontential crime networks \cite{Hsinchun2003}.
On the other hand, and more focused on the user side like our approach, there exist some user-centric solutions to problems like user authentication in a personal device, who Greenstadt and Beal \cite{cognitive_security_08} proposed to address using collected user biometrics along with machine learning techniques.
 
Then, when a Information Security Policy (ISP) is going to be applied, P.G. Kelley et al. \cite{user-controllable_learning_08} found important to include the user in the machine learning process for refining the policy model. They called it \textit{user-controllable policy learning}. Another approach to the refinement of user's privacy policies has been described by Danezis in \cite{inferring_policies_socialnetworks_09}, for he uses ML techniques over the user's settings in a social network, being capable of restricting permissions to other people depending on their interaction with the user.

In the same line, Lim et al. propose a system \cite{sec_policy_evolution_gp_08,pol_evol_gp_3_approaches_08} that evolves a set of computer security policies by means of GP, taking again into account the user's feedback. Furthermore, Suarez-Tangil et al. \cite{rule_generation_gp_09} take the same approach as Lim et al., but also bringing event correlation in. These two latter author's works are interesting for ours, though they are not focused on company ISPs - for instance, our case with the allowed or denied http requests -.

Finally, a system named MUSES (from Multiplatform Usable Endpoint
Security System) \cite{MUSES_SAC_14} is being developed under the
European Seventh Framework Programme (FP7). This system will include
event treatment on the user actions inside a company, DM techniques
for applying the set of policies from the company ISP to the actions,
allowing or denying them, and ML techniques for improving the set of
rules derived from these policies, according to user's feedback and
behaviour after the system decisions \cite{muses_sotics_13}.  


%%%%%%%%%%%%%%%%%%%%%%%%%%%%% PROBLEM DESCRIPTION %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%

\section{Problem and Data Description} 
\label{sec:problemDescription}

\noindent The problem to solve is related with the application of corporate security policies in order to deal with potential URL accesses inside an enterprise. To this end a dataset of URL sessions (requests and accesses) is analysed. These data are labelled with the corresponding permission or not for that access following the aforementioned rules. The problem is then transformed into a classification one, in which every new URL request will be classified, and thus, a grant or deny action will be assigned to that pattern.

The analysed data come from an \texttt{access.log} of the Squid
proxy application
\cite{squid:site}, in a real Spanish company. This open source tool
works as a proxy, but with the advantage of storing a cache of recent
transactions so future requests may be answered without asking the
origin server again \cite{DuaneWessels2004}.  
Every pattern, namely a URL session has ten variables associated,
which we describe in Table \ref{tabdata}, indicating if the variable
is numeric or nominal/categorical. 

\begin{table*}[htpb]
\centering
 \caption{\label{tabdata} Independent Variables corresponding to a URL session (a connection to a URL for some time). The URLs are parsed as detailed in Subsection \ref{subsec:logparsing}.}
{\scriptsize
\begin{tabular}{llll}
\hline\noalign{\smallskip}
Variable name & Description & Type & Rank/Number of Values (if categorical)\\
\noalign{\smallskip}\hline\noalign{\smallskip}
\texttt{http\_reply\_code} & Status of the server response & Categorical & 20 values\\
\texttt{http\_method} & Desired action to be performed & Categorical & 6 values\\
\texttt{duration\_milliseconds} & Session duration & Numerical & integer in [0,357170]\\
\texttt{content\_type} & Media type of the entity-body sent to the recipient & Categorical & 11 values (main content), 85 values (whole content)\\
\texttt{server\_or\_cache\_address} & IP address & Categorical & 2343 values\\
\texttt{time} & connection hour (in the day) & Date & 00:00:00 to 23:59:59\\
\texttt{squid\_hierarchy} & It indicates how the next-hop cache was selected & Categorical & 3 values\\
\texttt{bytes} & Number of transferred bytes during the session & Numerical & integer in [0,85135242]\\
\texttt{client\_address} & IP address & Categorical & 105 values\\
\texttt{URL} & Core domain of the URL, not taking into account the TLD & Categorical & 976 values\\
\noalign{\smallskip}\hline
\end{tabular}
}
\end{table*}

The dependent variable or class is a label which inherently assigns an decision (and so the following action) to every request. This can be: \textit{ALLOW} if the access is permitted according to the CSPs, or can be \textit{DENY}, if the connection is not permitted. These patterns are labelled using an `engine' based in a set of security rules, that specify the decision to make. This process is described in Subsection \ref{subsec:ruleparsing}.

These data were gathered along a period of two hours, from 8.30 to
10.30 am (30 minutes after the work started), monitoring the activity
of all the employees in a medium-size Spanish company (80-100 people),
obtaining 100000 patterns. We consider this dataset as quite complete because it contains a very diverse amount of connection patterns, going from personal (traditionally addressed at the first hour of work) to professional issues (the rest of the day).
Moreover, the results derived from the experiments (described in Section \ref{sec:results}) show that this quantity of data might be big enough, but a more accurate outcome would be given with, for instance, a 24 hours long log.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   METHODOLOGY  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{Methodology}
\label{sec:methodology}

\noindent Before classification techniques are applied, a data preprocessing step has been performed. First, the raw dataset is labelled according a set of \textit{initial corporate security rules}, i.e. every pattern is assigned to a label indication if the corresponding URL request/access would be ALLOWED or DENIED considering these rules. This step is necessary in order to transform the problem into a classification one. However, in order to apply the rules they must be transformed from their initial format into another one that can be applied in our programs (a hash in Perl\footnote{A \textit{hash} in Perl is an object that represents a \textit{hash table}, which is a set of pairs key-value. Sometimes, the value can be another hash itself.}). This is described in Subsection \ref{subsec:ruleparsing}. 

Subsection \ref{subsec:logparsing} details how the patterns of the navigation data log (URL sessions) are also converted to a Perl hash to perform the matting/labelling process. 

At the end of these two steps, the two hashes are compared in order to obtain which entries of the log should be ALLOW or DENY, know as the \textit{labelling} step. This is similar to perform a decision process in a security system. This step results in that there are 38972 pattern belonging to class ALLOW (positive class) and 18530 of class DENY (negative class), so just a 67.78\% of the samples belong to the majority class. This represents a very important problem, since a classifier that is trained considering these proportions is supposed to classify all the samples as ALLOW, getting a theoretically quite good classification accuracy equal or greater than 68\%. However, in section \ref{sec:results} we will see that, despite the fact that some denied patterns are classified as allow, the overall performance of the classifiers are better than the expected.

Given that the dataset contains a majority of categorical/nominal data, we have performed different approaches for data balancing:
\begin{itemize}
\item Undersampling: we will remove random samples of the majority class until the amount in both classes are similar.
\item Oversampling: we will duplicate random samples of the minority class, in order to get a close number of patterns in both classes. This has to be done due to the impossibility of creating synthetic data when dealing with categorical values (there is not a proper distance measure between two values in a category). Actually, since the number of samples in the majority class is almost twice the minority one, we have just duplicated all of those belonging to the minority class.
\end{itemize}

Finally, in Subsection \ref{subsec:methods} we explain the selection of the methods to apply in order to classify the data. We just have considered the patterns correctly labelled in the preprocessing phase. Thus, a supervised classification process \cite{classification_67} has been conducted on the balanced datasets.
Weka Data Mining Software\footnote{http://www.cs.waikato.ac.nz/ml/weka/} has been used, in order to select the best set of methods in order to deal with these data. These classifiers will be further tested in Section \ref{sec:results}.

% ------------------------------------------------------------------
%
\subsection{Security rules parsing}
\label{subsec:ruleparsing}

\noindent In this work we have considered Drools \cite{drools:site}
as the tool to create, and therefore, manage rules in a business environment. This so called Business Rule Management System (BRMS) has been developed by the JBoss community under an Apache License and it is written in Java. Though this platform consist of many components, here we focus on Drools Expert and the Drools Rule Language (DRL, \cite{drools:doc}). Then, the defined rules for a certain company are inside of a file with a \texttt{.drl} extension, the file that needs to be parsed to obtain the final set of rules. In Figure \ref{fig:drools_hash}, (a), there is the typical rule syntax in DRL. Two main things should be obtained from the parsing method: both left and right sides of the rule, taking into account that the left side is where the company specifies the conditions required to apply the action indicated in the right side. Also, for describing the conditions, Squid syntax is used (see Section \ref{sec:problemDescription}), having thus the following structure: \texttt{squid:Squid(\textit{conditions})}. Finally, from the right side of the rule, the \textit{ALLOW} or \textit{DENY} label to apply on the data that matches with the conditions, will be extracted. The Perl parser that we have implemented applies two regular expressions, one for each side of the rule, and returns a hash with all the rules with the conditions and actions defined. The `before and after' performing the parsing over the \texttt{.drl} file is in Figure \ref{fig:drools_hash}.

\begin{figure}[htb]
\centering
\subfloat[Drools Rule]{
\begin{tabular}{ p{0.05cm} p{0.05cm} p{2.5cm} }
  \texttt{rule~"name"} & & \\
   & \texttt{attributes} & \\
   & \texttt{when} & \\
   & & \texttt{/* Left Side of the Rule */} \\
   & \texttt{then} & \\
   & & \texttt{/* Right Side of the Rule */} \\
  \texttt{end} & & \\
\end{tabular}
}
~
\subfloat[Hash Rule]{
\begin{tabular}{ p{0.05cm} p{0.05cm} p{2.5cm} }
  \texttt{\%rules~=~(} & & \\
   & \texttt{rule~=>\{} & \\
   & & \texttt{field => xxx} \\
   & & \texttt{relation => xxx} \\
   & & \texttt{value => xxx} \\
   & & \texttt{action => [allow, deny]} \\
   & \texttt{\},} & \\
  \texttt{);} & & \\
\end{tabular}
}
\caption{(a) Structure of a rule in Drools Expert. (b) Resulting rule, after the parsing, in a global hash of rules. \label{fig:drools_hash}}
\end{figure}

% ------------------------------------------------------------------
%
\subsection{URL log data parsing}
\label{subsec:logparsing}

\noindent Usually, the instances of a log file have a number of fields, in order to have a registration of the client who asks for a resource, the time of the day when the request is made, and so on. In this case, we have worked with an \textit{access.log} (see Section \ref{sec:problemDescription}) file, converted into a CSV format file so it could be parsed and transformed in another hash of data. All ten fields of the Squid log yield a hash like the one depicted in Figure \ref{fig:data_hash}.

Once the two hashes of data were created, they were compared in such a way that for each rule in the hash of rules, it was determined how many entries in the data log hash are covered by the rule, and so they were applied the label that appears as `action' in the rule.

One of the problems was to extract from a whole URL the part that was
more interesting for our purposes. It is important to point out that
in a log with thousands of entries, an enormous variety of URLs can be
found, since some can belong to advertisements, images, videos, or
even some others does not have a domain name but are given directly by
an IP address. For this reason, we have taken into account that for a
domain name, many subdomains (separated by dots) could be considered,
and their hierarchy grows from the right towards the left. The highest
level of the domain name space is the Top-Level Domain (TLD) at the
right-most part of the domain name, divided itself in country code
TLDs and generic TLDs. Then, a domain and a number of subdomains
follow the TLD (again, from right to left). In this way, the URLs in the
used log are such as \textit{http://subdomain...subdomain.domain.TLD/}
\textit{other\_subdirectories}. However, for the ARFF\footnote{Format
  of Weka files} file to be created, only the domain (without the
subdomains and the TLD) should be considered, because there are too
many different URLs to take into consideration. Hence, applying
another regular expression, the data parser implemented in Perl
obtains all the core domains of the URLs, which makes 976 domains in
total. 

\begin{figure}[htb]
\centering
\begin{tabular}{ p{0.1cm} p{0.1cm} p{6cm} }
  \texttt{\%logdata~=~(} & & \\
   & \texttt{entry~=>\{} & \\
   & & \texttt{http\_reply\_code => xxx} \\
   & & \texttt{http\_method => xxx} \\
   & & \texttt{duration\_miliseconds => xxx} \\
   & & \texttt{content\_type => xxx} \\
   & & \texttt{server\_or\_cache\_address => xxx} \\
   & & \texttt{time => xxx} \\
   & & \texttt{squid\_hierarchy => xxx} \\
   & & \texttt{bytes => xxx} \\
   & & \texttt{url => xxx} \\
   & & \texttt{client\_address => xxx} \\
   & \texttt{\},} & \\
  \texttt{);} & & \\
\end{tabular}
\caption{Perl hash with an example entry. The actual hash used for this work has a total of 100000 entries, with more than a half labelled as \textit{ALLOW} or \textit{DENY} after the comparing process. \label{fig:data_hash}}
\end{figure}

% ------------------------------------------------------------------
%
\subsection{Classification Methods}
\label{subsec:methods}

% Ránking:
% Undersampling:             |      80% entrenamiento - 20% test (random)
%                            |
% 1- J48 97'02%              |                      97'38%
% 2- Random Forest 96'87%    |                      97'63%
% 3- REP Tree 96'79%         |                      97'41%
% 4- NNge 96'49%             |                      97'34%
% 5- PART 96'45%             |                      96'85%
%                            |
% Oversampling:              |      90% entrenamiento - 10% test (random)
%                            | 
% 1- J48 98%                 |                      97'56%
% 2- Random Forest 98'84%    |                      97'56%
% 3- REP Tree 97'67%         |                      97'56%
% 4- NNge 98'76%             |                      97'02%
% 5- PART 97'54%             |                      97'21%
%                            |


\noindent As said in Section \ref{sec:problemDescription}, the data used for this work is not only numerical or nominal, thus, only classification algorithms that support both types of data have been considered. Weka has a great number of possible algorithms to work with, so we have conducted a preselection phase trying to choose those which would yield better results in the experiments. More specifically, we have focused on rule-based and decision-tree-based algorithms. 

%It is important to point out that rules can become trees, but rules cannot always be derived from trees (for instance, a tree modelling a mathematical operation).

In this way, a decision-tree algorithm is a group of conditions organised in a top-down recursive manner in a way that a class is assigned following a path of conditions, from the root of the tree to one of its leaves. Generally speaking, the possible classes to choose are mutually exclusive. Furthermore, these algorithms are also called ``divide-and-conquer'' algorithms. On the other hand, there are the ``separate-and-conquer'' algorithms, which work creating rules one at a time, then the instances covered by the created rule are removed and the next rule is generated from the remaining instances.

A reference to each Weka classifier can be found at \cite{Frank2011}. Below are described the top five techniques, obtained from the best results (See Table \ref{tabresults_todos}) of the experiments done in this stage, along with more specific bibliography. Naïve Bayes method \cite{Bayesian_Classifier_97} has been included as a baseline, normally used in text categorization problems. According to the results, the five selected classifiers are much better than this method.

\begin{table}[htpb]
\centering
 \caption{\label{tabresults_todos} Results of all the tested classification methods on balanced data. The best ones are marked in boldface.}
{\small
\begin{tabular}{|l|l|l|}
\cline{2-3}
\multicolumn{1}{l|}{} & Undersampling & Oversampling \\ 
\hline
Naïve Bayes & 91.12 & 91.77 \\ 
\hline
Conjunctive Rule & 60.14 & 60.02 \\ 
\cline{1-1}
Decision Table & 94.08 & 90.29 \\ 
\cline{1-1}
DTNB & 94.75 & 95.65 \\ 
\cline{1-1}
JRip & 90.08 & 92.47 \\ 
\cline{1-1}
NNge & \textbf{96.49} & \textbf{98.76} \\ 
\cline{1-1}
One R & 93.45 & 93.70 \\ 
\cline{1-1}
PART & \textbf{96.45} & \textbf{97.54} \\ 
\cline{1-1}
Ridor & 87.22 & 89.87 \\ 
\cline{1-1}
Zero R & 51.39 & 51.26 \\ 
\cline{1-1}
AD Tree & 77.73 & 77.68 \\ 
\cline{1-1}
Decision Stump & 60.14 & 60.02 \\ 
\cline{1-1}
J48 & \textbf{97.02} & \textbf{98.00} \\ 
\cline{1-1}
LAD Tree & 79.95 & 79.97 \\ 
\cline{1-1}
Random Forest & \textbf{96.87} & \textbf{98.84} \\ 
\cline{1-1}
Random Tree & 95.14 & 98.35 \\ 
\cline{1-1}
REP Tree & \textbf{96.79} & \textbf{97.67} \\ 
\hline
\end{tabular}
}
\end{table}


\begin{description}
   \item[J48] This classifier generates a pruned or unpruned C4.5 decision tree. Described for the first time in 1993 by \cite{Quinlan1993}, this machine learning method builds a decision tree selecting, for each node, the best attribute for splitting and create the next nodes. An attribute is selected as `the best' by evaluating the difference in entropy (information gain) resulting from choosing that attribute for splitting the data. In this way, the tree continues to grow till there are not attributes anymore for further splitting, meaning that the resulting nodes are instances of single classes. 
   \item[Random Forest] This manner of building a decision tree can be seen as a randomization of the previous C4.5 process. It was stated by \cite{Breiman2001} and consist of, instead of choosing `the best' attribute, the algorithm randomly chooses one between a group of attributes from the top ones. The size of this group is customizable in Weka.
   \item[REP Tree] Is another kind of decision tree, it means Reduced Error Pruning Tree. Originally stated by \cite{Quinlan1987}, this method builds a decision tree using information gain, like C4.5, and then prunes it using reduced-error pruning. That means that the training dataset is divided in two parts: one devoted to make the tree grow and another for pruning. For every subtree (not a class/leaf) in the tree, it is replaced by the best possible leaf in the pruning three and then it is tested with the test dataset if the made prune has improved the results. A deep analysis about this technique and its variants can be found in \cite{Elomaa2001}.
   \item[NNge] Nearest-Neighbor machine learning method of generating rules using non-nested generalised exemplars, i.e., the so called `hyperrectangles' for being multidimensional rectangular regions of attribute space \cite{Martin1995}. The NNge algorithm builds a ruleset from the creation of this hyperrectangles. They are non-nested (overlapping is not permitted), which means that the algorithm checks, when a proposed new hyperrectangle created from a new generalisation, if it has conflicts with any region of the attribute space. This is done in order to avoid that an example is covered by more than one rule (two or more).
   \item[PART] It comes from `partial' decision trees, for it builds its rule set from them \cite{Frank1998}. The way of generating a partial decision tree is a combination of the two aforementioned strategies ``divide-and-conquer'' and ``separate-and-conquer'', gaining then flexibility and speed. When a tree begins to grow, the node with lowest information gain is the chosen one for starting to expand. When a subtree is complete (it has reached its leaves), its substitution by a single leaf is considered. At the end the algorithm obtains a partial decision tree instead of a fully explored one, because the leafs with largest coverage become rules and some subtrees are thus discarded.
 \end{description} 

These methods will be deeply tested on the dataset (balanced and unbalanced) in the following section.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   RESULTS  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{\uppercase{Experiments and Results}}
\label{sec:results}

\noindent Several experiments have been conducted, once a subset of classification methods has been chosen in previous section.
To this end, some training and test datasets have been created from
the set of labelled patterns. It contains 57502 samples, with 38972
belonging to class ALLOW and 18530 to class DENY.

In order to better test the methods, two different divisions (training-test) have been done, namely 90\%-10\% and 80\%-20\%. Moreover, two additional splits have been considered in every case, using both a random and a sequential approach for selecting samples from the original file. Thus, in the latter, consecutive patterns have been included in the training file up to the desired percentage. The rest have composed the test file. In the first approach, a random selection is performed.

The aim of the sequential division is to compare if the online activity of the employees considering URL sessions could be somehow `predicted', just using data from previous minutes or hours.

With respect to the data, the initial file was unbalanced, as it can be seen in the number of patterns per class. Hence, as stated in Section \ref{sec:problemDescription}, two data balancing methods have been applied to all the files, to get similar numbers in both classes: undersampling (random removal of ALLOW patterns) and oversampling (duplication of DENY patterns).

Results for unbalanced data are presented in Table \ref{tabresults_nobalan}.
Three different tests have been done for the random pattern distribution approach, so the mean and standard deviation are shown in the corresponding columns.

\begin{table*}[htpb]
\centering
 \caption{\label{tabresults_nobalan} Percentage of correctly classified patterns for non-balanced data}
{\small
\begin{tabular}{|l|l|l|l|l|}
\cline{2-5}
\multicolumn{1}{l|}{} & \multicolumn{2}{c|}{80\% Training - 20\% Test} & \multicolumn{2}{c|}{90\% Training - 10\% Test} \\ 
\cline{2-5}
\multicolumn{1}{l|}{} & Random (mean) & Sequential & Random (mean) & Sequential \\ 
\hline
J48 & 97.56 $\pm$ 0.20 & 88.48 & 97.70 $\pm$ 0.15 & 82.28 \\ 
\cline{1-1}
Random Forest & 97.68 $\pm$ 0.20 & 89.77 & 97.63 $\pm$ 0.13 & 82.59 \\ 
\cline{1-1}
REP Tree & 97.47 $\pm$ 0.11 & 88.34 & 97.57 $\pm$ 0.01 & 83.20 \\ 
\cline{1-1}
NNge & 97.23 $\pm$ 0.10 & 84.41 & 97.38 $\pm$ 0.36 & 80.34 \\ 
\cline{1-1}
PART & 97.06 $\pm$ 0.19 & 89.11 & 97.40 $\pm$ 0.16 & 84.17 \\ 
\hline
\end{tabular}
}
\end{table*}
 
As it can be seen, all the five methods achieved a high performance classifying in the right way the test dataset. Also, these results are not like this by chance, as shown by a low standard deviation. Although it was expected that the results from the 90\%-10\% division were slightly better, in the future a more aggressive division will be executed so the methods can be really proved with much less training data.

What matters to the results of the experiments made with the sequential data, they are worse than the obtained from the random data, but still they are good ($>$ 85\%). This is due to the occurrence of new patterns from a certain time (maybe there are some requests that are made just at one specific time in a day, or in settled days), and then there is no sufficient similarity between the training data and the classifying of the test data set may fail. The loss of 5 to 6 points in the results of the 90\%-10\% division is the first unexpected or unlogical result of the experiments, but they also reinforce the previous theory.

The technique that lightly stands out over the others is
\textit{Random Forest}, being the best in almost every case, even in
the experiments with the most complex sequential divisions. However,
if we focus on the standard deviation, \textit{REP Tree} is the chosen
one, as its results present robustness. 

For its part, results obtained from unbalanced data are shown in Table \ref{tabresults_balan}. Again the corresponding to the random partitions come from the mean of three blocks of experiments, and so are specified the standard deviations. The Table illustrates two segments of results, obtained from the undersampled data and from the oversampled data. For each one, the 90\%-10\% and 80\%-20\% divisions were also made.

% ***
% Se puede ver que todos los métodos obtienen un rendimiento similar en aciertos, siendo éste bastante alto. Si se mira la desviación estándard se ve que los resultados no son fortuitos, ya que ésta es muy pequeña.
% 
% Como era de esperar la división en 90\%-10\% mejora los resultados, aunque no mucho, por lo que habría que hacer una prueba con una división más agresiva (trabajo futuro) para saber las capacidades de los métodos al trabajar con pocos datos de entrenamiento.
% 
% Respecto a los datos secuenciales sus resultados son peores, pero aún así bastante buenos (> 85\%).
% La bajada en aciertos se debe sin duda a la aparición de patrones totalmente nuevos a partir de determinada hora (puede que algo que se haga de forma programada cada día o determinados días), por lo que no hay patrones similares con los que entrenar y se falla al clasificar en el test.
% 
% Los aciertos son incluso menores (hay una bajada de 5-6 puntos) al hacer una división considerando más patrones, contrariamente a lo esperado/logico, por lo que esto refuerza la teoría de patrones que sólo suceden a determinadas horas.
% 
% Como técnica destaca levemente \textit{Random Forest}, siendo la que mejores resultados obtiene en casi todos los casos, incluyendo las más complejas particiones secuenciales. Aunque mirando la desviación estándar nos decantamos por \textit{REP Tree}, que obtiene resultados más robustos.
% ***
% 
% ***
% Los resultados de los datos balanceados se muestran en la Tabla \ref{tabresults_balan}. Nuevamente los resultados de las particiones aleatorias se muestran en media de tres particiones.
% ***

\begin{table*}[htpb]
\centering
 \caption{\label{tabresults_balan} Percentage of correctly classified patterns for balanced data (under- and oversampling)}
{\small
\begin{tabular}{|l|l|l|l|l|l|l|l|l|}
\cline{2-9}
\multicolumn{1}{l|}{} & \multicolumn{4}{c|}{80\% Training - 20\% Test} & \multicolumn{4}{c|}{90\% Training - 10\% Test} \\ 
\cline{2-9}
\multicolumn{1}{l|}{} & \multicolumn{2}{c|}{Undersampling} & \multicolumn{2}{c|}{Oversampling} & \multicolumn{2}{c|}{Undersampling} & \multicolumn{2}{c|}{Oversampling} \\ 
\cline{2-9}
\multicolumn{1}{l|}{} & Rand (mean) & Sequential & Rand (mean) & Sequential & Rand (mean) & Sequential & Rand (mean) & Sequential \\ 
\hline
J48 & 97.05 $\pm$ 0.25 & 84.29 & 97.40 $\pm$ 0.03 & 85.66 & 96.85 $\pm$ 0.35 & 76.44 & 97.37 $\pm$ 0.06 & 74.24 \\ 
\cline{1-1}
Random Forest & 96.61 $\pm$ 0.17 & 88.59 & 97.16 $\pm$ 0.19 & 89.03 & 96.99 $\pm$ 0.13 & 79.98 & 97.25 $\pm$ 0.33 & 81.33 \\ 
\cline{1-1}
REP Tree & 96.52 $\pm$ 0.13 & 85.54 & 97.13 $\pm$ 0.25 & 85.41 & 96.55 $\pm$ 0.10 & 77.65 & 97.14 $\pm$ 0.09 & 76.81 \\ 
\cline{1-1}
NNge & 96.56 $\pm$ 0.42 & 85.28 & 96.90 $\pm$ 0.28 & 83.46 & 96.33 $\pm$ 0.05 & 81.93 & 96.91 $\pm$ 0.06 & 78.73 \\ 
\cline{1-1}
PART & 96.19 $\pm$ 0.14 & 85.16 & 96.82 $\pm$ 0.09 & 84.50 & 96.09 $\pm$ 0.10 & 79.70 & 96.68 $\pm$ 0.11 & 78.16 \\ 
\hline
\end{tabular}
}
\end{table*}

\begin{description}
  \item[Applying Undersampling] In comparison with those results from Table \ref{tabresults_nobalan}, these go down one point (in the case of randomly made divisions) to six points (sequential divisions). The reason why this happens is that when randomly removing ALLOW patterns, we are really losing information, i. e. key patterns that could be decisive in a good classification of a certain set of test patterns. 
  \item[Applying Oversampling] Here we have duplicated the DENY patterns so their number could be up to that of the ALLOW patterns. However, it does not work as well as in other approaches which uses numerical computations for creating the new patterns to include in the minority class. Consequently, the results have been decreased.
\end{description}

In both cases it is noticeable that taking the data in a sequential way, instead of randomly, lower the results. It is clear that due to the fact that performing undersampling some patterns are lost while in the case of oversampling they all remain, \textit{undersampling results} are better. Then, in this case the algorithm with best performance is \textit{J48}, though \textit{Random Forest} follows its results very closely in random datasets processing, and \textit{REP Tree}, which is better than the rest when working with sequential data. Nevertheless, generally speaking and given the aforementioned reasons, performing data balancing methods yields worse results.

Furthermore, we have found that for the data sets taken consecutively, the methods always classify worse the DENY labels, as they label them as ALLOW patterns. This is worth further study because it is the worst situation. It would be preferable to have a false positive in a DENY pattern, rather than a false negative and permit a request that is forbidden in the ISP.

Regarding the obtained rules/trees, we want to remark that the majority are based on the URL in order to discriminate between the two classes, however we also found several ones which consider variables/features different of this to make the decision. For instance:\\

\begin{verbatim}
IF server_or_cache_address = "90.84.53.17" 
THEN DENY

IF server_or_cache_address = "173.194.78.103" 
THEN ALLOW

IF content_type = 
 "application/vnd.google.safebrowsing-update" 
THEN DENY

IF server_or_cache_address = "173.194.78.94" 
AND content_type_MCT = "text"
AND content_type = "text/html"
AND http_reply_code = "200"
AND bytes > 772
THEN ALLOW

IF server_or_cache_address = "173.194.34.225"
AND http_method = "GET"
AND duration_milliseconds > 52
THEN ALLOW

IF server_or_cache_address = "90.84.53.49"
AND time <= 33758000
THEN ALLOW
\end{verbatim}

These are the interesting rules for our purposes, since they are somehow independent of the URL to which the client requests to access. Thus, it would be potentially possible to allow or deny the access to unknown URLs just taking into account some parameters of the session.

Of course, some of these features depend on the session itself, i.e. they will be computed after the session is over, but the idea in that case would be 'to refine' somehow the existing set of URLs in the White List.
Thus, when a client requests access to a Whitelisted URL, this will be allow, but after the session is over, and depending on the obtained values, and on one of these classifiers, the URL could be labelled as DENIED for further requests.
This could be a useful decision-aid tool for the CSO in a company, for instance.
In the case that the features considered in the rule can be known in advance, such as \texttt{http\_method}, or \texttt{server\_or\_cache\_address}, for instance, the decision could be made in real-time, and thus, a granted URL (Whitelisted) could be DENIED or the other way round.

The tree-based methods also yield several useful branches in this sense, but they have not been plotted here because of the difficulty for showing/visualizing them properly.

%
%server_or_cache_address = 90.84.53.80: deny (27.0/1.0)
%
%server_or_cache_address = 208.43.111.194: deny (19.0)
%
%server_or_cache_address = 173.194.78.105 AND
%content_type_MCT = null: allow (19.0)
%
%server_or_cache_address = 173.194.78.94 AND
%content_type_MCT = text AND
%content_type = text/html AND
%http_reply_code = 200 AND
%bytes > 772: allow (42.0)
%
%
%server_or_cache_address = 173.194.78.106 AND
%content_type_MCT = image: allow (8.0)
%
%server_or_cache_address = 173.194.34.225 AND
%http_method = GET AND
%duration_milliseconds > 52: allow (51.0/1.0)
%
%server_or_cache_address = 90.84.53.49 AND
%time <= 33758000: allow (33.0/1.0)
%
%server_or_cache_address = 173.194.78.104: allow (21.0/3.0)
%
%server_or_cache_address = 173.194.78.94 AND
%content_type = text/plain: allow (15.0)
%
%server_or_cache_address = 91.216.63.241: allow (10.0/2.0)
%
%server_or_cache_address = 66.196.66.157 AND
%duration_milliseconds > 250: allow (8.0)
%
%server_or_cache_address = 176.28.113.132 AND
%time > 33570000: deny (21.0/2.0)
%
%server_or_cache_address = 173.194.78.94 AND
%content_type = text/html AND
%duration_milliseconds > 52 AND
%http_reply_code = 200 AND
%duration_milliseconds <= 72: allow (14.0)
%



% ***
% Como puede verse en la tabla...\\
% 
% A DESARROLLAR:\\
% Si no está escrito debéis ponerlo en comentarios. - JJ 

% * Undersampling peor que Oversampling - lógico porque se pueden perder patrones que pueden ser relevantes y en el otro se mantienen todos.\\
% 
% * Resultados empeoran al usar las técnicas de balanceo. La Justificación sería algo como esto:\\
% 
% DON'T WORK PROPERLY:\\
% Undersampling - Losses important information (key patterns).\\
% Oversampling - Just duplication because there are several nominal values so it doesn't work as well as in other approaches which uses numerical computations for creating the new patterns to include in the minority class.\\
% Sequential - Some kind of URL accesses are just performed at a time, which could not be included in the training set.\\
% 
% * Resultados secuenciales peor que aleatorios por lo explicado antes. Aquí incluso empeoran más porque se pierde más información que antes\\
% 
% En este caso destacan los algoritmos \textit{J48}, muy cercano a \textit{Random Forest} en el procesamiento de ficheros aleatorios, y \textit{REP Tree}, que mejora a todos los demás al trabajar con datos secuenciales.
% ***\\

And now we see in \ref{tab_12featvs9feat} and \ref{tab_featselect} that our results are awesome and your argument is invalid.

\begin{table}[htpb]
\centering
 \caption{\label{tab_12featvs9feat} Comparison between obtained accuracies for the initial data set, which had 12 features, and the resulting after applying Rough Set Theory Feature Selection, having 9 features.}
{\small
\begin{tabular}{|l|c|c|}
\cline{2-3}
\multicolumn{1}{l|}{} & 12 features & 9 features \\ 
\hline
Naïve Bayes & 92.30 & 92.19 \\ 
\cline{1-1}
JRip & 92.95 & 91.66 \\ 
\cline{1-1}
NNge & 97.18 & 97.13 \\ 
\cline{1-1}
PART & 97.41 & 97.27 \\ 
\cline{1-1}
J48 & 97.37 & 97.40 \\ 
\cline{1-1}
Random Forest & 97.60 & 97.57 \\
\cline{1-1}
REP Tree & 97.33 & 97.34 \\ 
\hline
\end{tabular}
}
\end{table}

\begin{table*}[htpb]
\centering
 \caption{\label{tab_featselect} Percentage of correctly classified patterns for non-balanced data}
{\small
\begin{tabular}{|l|c|c|c|c|}
\cline{2-5}
\multicolumn{1}{l|}{} & \multicolumn{2}{c|}{80\% Training - 20\% Test} & \multicolumn{2}{c|}{90\% Training - 10\% Test} \\ 
\cline{2-5}
\multicolumn{1}{l|}{} & Random (mean) & Sequential & Random (mean) & Sequential \\ 
\hline
J48 & 96.97 $\pm$ 0.17 & 87.83 & 97.09 $\pm$ 1.03 & 84.51 \\ 
\cline{1-1}
Random Forest & 97.68 $\pm$ 0.67 & 88.06 & 97.80 $\pm$ 1.17 & 83.71 \\ 
\cline{1-1}
REP Tree & 97.14 $\pm$ 0.10 & 87.79 & 97.18 $\pm$ 0.71 & 85.73 \\ 
\cline{1-1}
NNge & 96.27 $\pm$ 0.14 & 82.18 & 97.27 $\pm$ 1.57 & 80.94 \\ 
\cline{1-1}
PART & 97.17 $\pm$ 0.05 & 87.88 & 97.08 $\pm$ 1.14 & 85.11 \\ 
\hline
\end{tabular}
}
\end{table*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  CONCLUSIONS  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{\uppercase{Conclusions and Future Work}}
\label{sec:conclusions}

\noindent In this paper a set of classification methods have been applied
in order to perform a decision process inside a company, according to
some predefined corporate security policies. 
This decision is focused on allowing or denying URL access requests, but just considering previous decisions on similar requests, not having specific rules in a White/Black List, defined for those URLs. Thus, the proposed method could allow or deny an access to a URL based in additional terms rather than just the specific URL string. This could be very useful since new URLs could be automatically 'Whitelisted' or 'Blacklisted', just depending on some of the connection parameters, such as the \texttt{content\_type} of the access or the \texttt{IP} of the client which makes the request.

To this aim, we have started from a big dataset (100000 patterns) about employees' URL sessions information, and considering a set of URL access permissions, we have composed a labelled dataset (57000 patterns). Over that set of data, we have tested several classification methods, after some data balancing techniques have been applied. Then, the best five have been deeply proved over several training and test divisions, and with two methods: using sequential patterns (consecutive URL accesses), and taking them in a randomly way.

The results show that classification accuracies are between 95\% and 97\%, even when using the unbalanced datasets. However, they have been diminished because of the possible loss of data that comes from performing an undersampling (removing patterns) method; or taking the training and the data sets in a sequential way from the main log file, due to the fact that certain URL requests can be made only at a certain time.

In this way, we can conclude that the approach has been successful and it would be a useful tool in an enterprise.

% ***
% RESUMEN en un párrafo de lo que se ha obtenido:
% - muy buenos porcentajes (en torno al 95-97\%) en general incluso en datos no balanceados
% - el método pierde efectividad si se omiten algunos patrones que pueden no haber aparecido antes (datos secuenciales o undersampling) => hay que estudiarlo mejor en siguientes trabajos
% 
% Esto teníais que haberlo puesto antes que nada. - JJ
% Antonio - yo creo que está bien así. Las conclusiones suelen ser autocontenidas y yo las estructuro como un resumen del trabajo. ;)
% So we can conclude that...
% *** la propuesta ha sido exitosa y podría ser, potencialmente, una buena herramienta de decisión en sistemas de seguridad ***
Future lines of work include conducting a deeper set of experiments trying to test the generalisation power of the method, maybe considering bigger data divisions, bigger data sets (from a whole day or working day), or adding some kind of `noise' to the dataset.
So that, considering the good classification results obtained in this work, the next step could be the application of these methods in the real system from which data was gathered, counting with the opinion of expert CSOs, in order to know the real value of the proposal.
The study of other classification methods could be another research branch, along with the implementation of a Genetic Programming approach, which could deal with the imbalance problem using a modification of the cost associated to misclassifying, could be done (as the authors did in \cite{cost_adjustment_07}).

Finally, we also point to extract additional information from the URL string, than could be transformed into additional features that could be more discriminative than the current set. Moreover, a data process involving summarizing data about sessions (such as number of requests per client, or average time connection) will be also considered.



\section*{Acknowledgements}
This paper has been funded in part by European project MUSES (FP7-318508), along with Spanish National project TIN2011-28627-C04-02 (ANYSELF), project P08-TIC-03903 (EVORQ) awarded by the Andalusian Regional Government, and projects 83 (CANUBE), and GENIL PYR-2014-17, both awarded by the CEI-BioTIC UGR.

\bibliographystyle{splncs03}
\bibliography{data_mining_urls}


\end{document}
