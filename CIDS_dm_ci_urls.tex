\documentclass{llncs}

\usepackage[latin1]{inputenc}
\usepackage{epsfig}
\usepackage{graphicx}        % standard LaTeX graphics tool
\usepackage{url}
\usepackage{subfig}
\usepackage{calc}
\usepackage{makeidx}
\usepackage{amsmath}

\usepackage{multicol}
\usepackage{pslatex}
\usepackage{caption}
\usepackage{amssymb}
\usepackage{verbatim}
\usepackage{algorithm,algorithmic}
\usepackage{soul}


\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   TITLE   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{An Improved Decision System for URL Accesses based on a Rough Feature Selection Technique}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   AUTHORS   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\author{P. de las Cuevas, Z. Chelly, A.M. Mora, J.J. Merelo, A.I. Esparcia-Alc\'azar}
\authorrunning{P. de las Cuevas et al.}

\institute{Department of Computer Architecture and Computer Technology, University of Granada, Spain. \\
Laboratoire de Recherche Op\'erationelle de D\'ecision et de Contr\^ole de Processus, Institut Sup\'erieur de Gestion, Tunisia.\\
S2 Grupo, Spain\\
% problem with letters here. You should probably either use LaTeX symbols for that or change to UTF8 - JJ
% Ok, final version will be stored with the appropriate encoding - Paloma
% Antonio - I'm fixing the symbols. Zeinebtry to change your editor configuration.
 {\tt \{paloma,amorag,jmerelo\}@geneura.ugr.es, zeinebchelly@yahoo.fr, aesparcia@s2grupo.es}
}

\maketitle

%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   ABSTRACT   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\begin{abstract}
Corporate security is usually one of the matters in which companies
invest more resources, since the loss of information directly
translates into monetary losses. Security issues  might have an origin
in external attacks or internal security failures, but an important
part of the security breaches is related to the lack of awareness that
the employees have with regard to the use of the Web. In this work we
have focused on the latter problem, describing the improvements to a system
able to detect anomalous and potentially insecure situations that
could be dangerous for a company. This system was initially conceived
as a better alternative to what are known as black/white lists. These
lists contain URLs whose  access is banned or dangerous (black
list), or URLs to which the access is permitted or allowed (white
list).
In this chapter, we propose a system that can initially learn from
existing black/white lists and then classify a new, unknown, URL request
either as ``should be allowed'' or ``should be
denied''. This system is described, as well as its results and the
improvements made by means of an initial data pre-processing step based on applying Rough Set Theory for feature selection. We prove that
high accuracies can be obtained even without including a pre-processing step, reaching between 96\% and 97\% of correctly classified
patterns. Furthermore, we also prove that including the use of Computational Intelligence techniques for pre-processing the data enhances the system performance, in terms of running time, while the accuracies remain close to 97\%.
%while before we only obtained... - JJ
%Thing is that we already obtained those accuracies... it's mentioned right after, maybe it's better to re-write...
Indeed, among the obtained results, we demonstrate that it is possible to obtain interesting rules which are not based only on the URL string feature, for classifying new unknown URLs access requests as allowed or as denied.
\end{abstract}
% Antonio - I think that the asbtract is a bit long.

\textbf{Keywords.} Computational Intelligence, Rough Sets, Feature Selection, Corporate Security Policies, Internet Access Control, Data Mining, Blacklists and Whitelists.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   INTRODUCTION   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{Introduction}
\label{sec:introduction}

\noindent Security is an inclusive term that refers to a diversity of steps taken by individuals,
and companies, in order to protect computers or computer networks that are connected to the Internet.
The Internet was initially conceived as an open network facilitating the free exchange of information.
However, data which is sent/received over the Internet travel through a dynamic chain of computers and
network links and, as a consequence, the risk of intercepting and changing the data is high.
In fact, it would be virtually impossible to secure every computer connected to the Internet around the world.
So, there will likely always be weak links in the chain of data exchange \cite{cheswick2003firewalls}.
Yet, companies have to find out a way for their employees to safely interact with customers, clients, and anyone
who uses the Internet while protecting internal confidential information. Companies have, also, to alert the employees
from the Internet misuse while doing their job.

Most of the time, employees have a misguided sense of security and believe that it is an IT problem, a purely technical
issue, and they naively believe that an incident may never happen to them \cite{stanton2005analysis}.
Actually, the employees' web misuse is one of the main causes of  security breaches \cite{breivik2002abstract},
so that making them security-conscious has become a security  challenge.

The reality is that every department must be involved in  readiness planning and establishing security policies
and procedures  to minimize their risks. Such strategies are mainly handled by
means  of \textit{Corporate Security Policies} (CSPs) which basically are a  set of security rules aiming at
protecting company assets by defining  permissions to be considered for every different action to be  performed
inside the security system \cite{kaeo2003designing}.

The basic idea behind these CSPs is usually to include rules to either allow or deny employees' access to
non-confident or non-certified websites, which are referenced by their URLs in this chapter. Moreover, several web pages
might be also controlled for productivity  or suitability reasons, given the fact that the employees who connect
to these might have working purposes or not. In fact, some of the  CSPs usually define sets of allowed or denied
web pages or websites that could be accessed by the company employees. These sets are  usually included in two main
lists; a white list (referring to  ``permitted'') and a black list (referring to  ``non-permitted'').
Both lists, the white and the black, act as a  good and useful control tools for those URLs included in them,
as  well as for the complementary. For instance, the URLs which are not included in a white list will automatically
have a denial of  access \cite{ludl2007effectiveness}.

The aim of this paper is going beyond this traditional and simple decision making process. By using black
and/or white lists, we either allow or deny users' requests/connection based, only, on the URLs provided in the lists.
Yet, updating these lists is a never ending task, as numerous malicious websites appear every day.
For instance, Netcraft reports from November of 2014 \cite{netcraft:site} showed that there are about 950
million active websites. But McAfee reported \cite{mcafee:site} that, at the end of the first quarter of 2014,
there were more than 18 million new suspect URLs (2 million associated domains), and also more than 250 thousand
new phishing URLs (almost 150 thousand associated domains).

With this situation in mind, in this chapter, our aim is to define a tool for automatically making allow or
deny decisions with respect to URLs that are not included in the aforementioned lists. This decision
would be based on that made for similar URL accesses (those with similar features), but instead of using only
the URL strings included in the lists, we will consider other parameters of the request/connection.

For this reason, the problem has been mapped to a \textit{classification} problem in which we start from a set of
unlabelled patterns that model the connection properties from a huge amount of \textit{actual}\footnote{Taken from a
log file released to   us by a  Spanish company.} URL accesses, known as sessions. After that, we assign a label to many
of them, considering a set of \textit{actual}\footnote{The set of rules has been written by the same   company, with respect
to its employees.} security rules (CSPs) defined by the Chief Security Officer (CSO) in the company. This was the approach
followed in \cite{ECTA}, and which we extend in this chapter.

In order to extract conclusions from the resulting studied dataset and to properly apply a classification algorithm,
a pre-processing step is needed. In fact, to obtain an accurately trained classifier, there is a need to extract as much
information as possible from the connections
that the employees normally make throughout the workday. This translates into high computational requirements, which is
why we introduce in this paper  techniques for data reduction. More precisely, we aim to apply a feature selection
technique to extract the most important features from the data at hand. Among the well known feature selection techniques
proposed in literature, we propose to use a Computational Intelligence method: the Rough Set Theory (RST) \cite{pawlak2008rough}. RST has been experimentally
evaluated with other leading feature selection techniques, such as Relif-F and entropy-based approaches
in \cite{jensen2007fuzzy}, and has been shown to outperform these in terms of resulting classification performance.

After pre-processing and based on the reduced dataset, we will apply several classification algorithms,
testing them and selecting the most appropriate one for this problem. The selected classifier should be capable
of dealing with our data while producing high accuracies and
being lightweight in terms of running time. Moreover, as we want to further test the reliability of the results,
in this work we propose different experimental setups based on different data
partitions. These partitions are formed either by preserving the order of the data or by taking the patterns in
a random way. Finally, given that the used data presents unbalance, we aim to apply balancing
techniques \cite{imbalance_techniques_02} to further guarantee the fairness of our obtained results.

In this chapter, we want to improve the accuracies obtained in our previous work \cite{ECTA},
as well as see if the new incorporated method (namely Rough Sets) for feature selection yields to better rules.
This is meant to be done not only by applying RST for feature selection, but also by improving the quality of the
original data set, by means of erasing information that may be redundant.

The rest of the paper is structured as follows. Next section describes the state of the art related
to Data Mining (DM), Machine Learning (ML), and Computational Intelligence (CI) techniques applied
to corporate security. Also, related works about URL filtering will be reviewed. Data  description is
detailed in Section \ref{sec:problemDescription}. Then, Section \ref{sec:featureselection} describes the
basic concepts of Rough Set Theory for feature selection which we  have used for data pre-processing.
Section \ref{sec:methodology} gives an overview of the  followed methodology, as well as the improvements
done after our  first results obtained in \cite{ECTA}. Then Section \ref{sec:results} depicts the results, and discusses the  obtained rules which are different for every used classifier. Finally, conclusions and future trends are given in Section \ref{sec:conclusions}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  STATE OF THE ART  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{State of the Art}
\label{sec:stateofart}

\noindent Our work tries to obtain a URL classification tool for
enhancing the security in the client side, as at the end we want to
get if a certain URL is secure or not, having as reference a set of
rules (derived from a CSP) which allow or deny a set of known
\textit{HTTP} requests. For this, different techniques belonging to
Data Mining (DM), Machine Learning (ML), and Computational
Intelligence, have been applied. This section gives  an overview in a
number of solutions given to protect the user, or the company, against
insecure situations.

First, as we want to add a good pre-processing phase to our system, in order to improve it, Subsection \ref{subsec:dataanalysis} gives an overview of the state of the art related to data analysis and pre-processing. Then, in Subsection \ref{subsec:relatedworks} we try to analyse similar systems, as well as define which advantages our system provides.

% ------------------------------------------------------------------
\subsection{Data Analysis and pre-processing}
\label{subsec:dataanalysis}

Performing DM means analyzing the database we have \cite{Frank2011}
which in our case is a log of HTTP requests. The work discussed in \cite{wilson2001maintaining} presents an exhaustive review of works which study database cleaning and their conclusion is that a database with good quality is decisive when trying to obtain good accuracies; a fact which was also demonstrated in \cite{zeineb2014thesis}. To analyse the data that we have at hand, we have based our work on two main processes: data pre-processing on the URL dataset and the application of balancing techniques depending on the data.

While performing data pre-processing, we have focused first on the kind/type of data included in the HTTP requests in the log file that is used as input file. We realised  that many URL strings are redundant in the dataset and thus we aimed  to eliminate them which is seen a cleaning approach.
 %maintain? keep? Maintain it how? - JJ
% sentence reformulated -- Zeineb
%The word "maintenance" is replaced by "cleaning" in all the manuscript -- Zeineb
  
%The quality of the dataset is very important to  generate accurate results and to have the possibility to learn models  from the presented data. 
% repeated from previous paragraph, maybe eliminate  JJ
% sentence deleted -- Zeineb 

% Thus, maintaining it is an essential  step. 
% maintaining quality?  Maintaining with respect to what?
% Please rephrase
% sentence deleted -- Zeineb 

Many cleaning techniques have been proposed in literature
\cite{wilson2001maintaining} in order to guarantee the good quality of
a given dataset.   Most of these techniques are based on updating a
database by adding or deleting instances to optimize and reduce the
initial database.  % rather than maintaining, wouldn't that be
                   % obtaining? Cleaning? - JJ
                   %The word "maintenance" is replaced by "cleaning" in all the manuscript -- Zeineb
These policies include different operations such as deleting the
outdated, redundant, or inconsistent instances; merging groups of
objects to eliminate redundancy and improve reasoning power;
re-describe objects to repair incoherencies; check for signs of
corruption in the database and controlling any abnormalities in the
database which might signal a problem. Working with a database which is not
cleaned % cleaned? preserved? cleansed? debugged? - JJ
%The word "maintenance" is replaced by "cleaning" in all the manuscript -- Zeineb
can become sluggish % sluggish as in slow? Working with it might be
                    % sluggish, not the database itself - JJ
                    %sentence reformulated -- Zeineb
and without accurate data users will make uninformed decisions. In
this work, we have maintained the our HTTP request dataset by focusing
on a specific kind of data that should be eliminated:
redundant URL strings. Section \ref{subsec:duplicateddata} explains in
detail the process that we have adopted to eliminate these redundant
data.

Still with the data pre-processing task, we have focused as a second
step on checking the importance of the %doing what to them? - JJ
%sentence reformulated -- Zeineb
  set of features presented in the HTTP requests log file. Thus
 we tried to select the most informative features from the initial
 feature set. At this point, we have introduced an extra technique, a
 data reduction technique, that was not included in our first work
 presented in \cite{ECTA}.  % you should use the standard
                            % bibliography, not your own for our
                            % papers - JJ
% I think the reference is correct JJ -- Zeineb
Feature reduction % data or feature reduction? - JJ
% reformulated -- Zeineb
is a main point of interest across a wide
variety of fields and focusing on this step is crucial as it
often presents a source of significant information loss. % what? Reducing
                                % data means data loss? - JJ
                                % it is seen as an information loss -- Zeineb
Many techniques were
proposed in literature to achieve the task of feature reduction %without
                                %data loss? - JJ
                                % no, here we talk in general -- Zeineb
 and they
can be categorized into two main heads; techniques that transform the
original meaning of the features, called the ``transformation-based
approaches", and the second category is a set of semantic-preserving
techniques known as the ``selection-based approaches".

Transformation based approaches, also called ``feature extraction
approaches", involve simplifying the amount of resources required to
accurately describe a large set of data. Feature extraction is a
general term for methods that construct combinations of variables to
represent the original set of features but with new variables while
still describing the data with sufficient accuracy. The transformation
based techniques are employed in situations where the semantics of the
original database will not be needed by any future process. In
contrast to the semantics-destroying dimensionality reduction
techniques, the semantics-preserving techniques, also called ``feature
selection techniques", attempt to retain the meaning of the original
feature set. The main aim of this kind of techniques is to determine a
minimal feature subset from a problem domain while retaining a
suitably high accuracy in representing the original features
\cite{liu1998feature}. In this work, we mainly focus on the use of a
feature selection technique, instead of a feature extraction
technique, as it is crucial to preserve the semantics of the features
in the URL data that we dispose at hand, and among them, select the
most important/informative ones which nearly preserve  the same
performance as the initial feature set.

Yet it is important to mention that most feature selection techniques
proposed in the literature suffer from some limitations. Most of these
techniques involve the user for the task of the algorithms
parameterization and this is  seen as a significant
drawback. Some feature selectors require noise levels to be specified
by the user beforehand, some simply rank features leaving the user to
choose their own subset. There are those that require the user to
state how many features are to be chosen, or they must supply a
threshold that determines when the algorithm should terminate. All of
these require the user to make a decision based on its own (possibly
faulty) judgment \cite{jensen2005semantics}. To overcome the
shortcomings of the existing methods, it would be interesting to look
for a method that does not require any external or additional
information to function appropriately. Rough Set Theory (RST)
\cite{pawlak2008rough}, which will be deeply explained in Section
\ref{sec:featureselection}, can be used as such tool.

As previously stated and apart from applying a data pre-processing
process, we aim to apply balancing techniques, depending on the
distribution of patterns per class, in order to ensure the fairness of
our results. This is due to the fact that using ``real
data''\footnote{Data which was gathered from the real world, and was
  not artificially generated.} may yield to highly unbalanced data
sets \cite{imbalanced_data_05}. This is our case, as the log file
includes a set of URL accesses performed by humans, and indeed we
obtained an unbalanced dataset. In order to deal with this kind of
data there exist several methods in literature known as balancing
techniques \cite{imbalanced_data_05}. These methods can be categorized
into three main groups \cite{imbalance_techniques_02}:

\begin{itemize}
\item \textit{Undersampling the over-sized classes}: This category aims at reducing the considered number of patterns for the classes with the majority.
\item \textit{Oversampling the small classes}: This category aims at introducing additional (normally synthetic) patterns in the classes with the minority.
\item \textit{Modifying the cost associated to misclassifying the positive and the negative class}: This category aims at compensating the unbalance in the ratio of the two classes. For example, if the imbalance ratio is 1:10 in favour of the negative class, the penalty of misclassifying a positive example should be 10 times greater.
\end{itemize}

Techniques belonging to the first group have been applied to some works, following a random undersampling approach \cite{random_undersampling_08}. However, those techniques have the problem of the loss of valuable information.

Techniques belonging to the second  group have been so far the most widely used, following different approaches, such as SMOTE (Synthetic Minority Oversampling Technique)  which is a method proposed in \cite{smote_02} for creating `artificial' samples for the minority class in order to balance the amount of them, with respect to the amount of samples in the majority class. However this technique is based on numerical computations, considering  different distance measures, in order to generate useful patterns  (i.e. realistic or similar to the existing ones).

The third group implies using a method in which a cost can be associated to the classifier accuracy at every step. This was done for instance  in \cite{cost_adjustment_07}, where  a Genetic Programming (GP) approach was used in which the fitness function was modified in order to consider a penalty when the classifier makes a false negative (an element from the minority class was classified as belonging to the majority class).
However almost all the approaches deal with numerical (real, integer)
data.

For our purposes, we will focus on techniques of the first and second group, as we will use state-of-the-art classifiers. Details about the balancing techniques used in our work will be explained in Section \ref{subsec:balancing}.

% ------------------------------------------------------------------
%
\subsection{Related work and contribution}
\label{subsec:relatedworks}

The works that we are interested in are those which scope is related with the users' information and behaviour, and the management (and adaptation) of Information or Corporate Security Policies (ISPs).

In this line, in \cite{cognitive_security_08} a combined biometrics signals with ML methods in order to get a reliable user authentication in a computer system was proposed. In \cite{user-controllable_learning_08}  a method was presented named \textit{user-controllable policy learning} in which the user gives feedback to the system every time that a security policy is applied, so these policies can be refined according to that feedback to be more accurate with respect to the user's needs.

On the other hand, policies could be created for enhancing user's privacy, as proposed  in \cite{inferring_policies_socialnetworks_09}, where a system able to infer privacy-related restrictions by means of a ML method applied in a social network environment was defined. The idea of inferring policies can be also considered after our results, given the fact that we are able to obtain new rules from the output of the classifiers, but in the scope of the company, and focused on ISPs.

In the same line, in \cite{sec_policy_evolution_gp_08,pol_evol_gp_3_approaches_08}  a system  was proposed which evolves a set of computer security policies by means of GP, taking again into account the user's feedback. Furthermore, the work presented in  \cite{rule_generation_gp_09} took the same approach as the latter mentioned work, but also bringing event correlation into it. The two latter works are interesting in our case, though they are not focused on company ISPs; for instance, our case with the allowed or denied HTTP requests.

Furthermore, it is worth mentioning a tool developed in  \cite{harris2003next}, taking the approach of ``greylisting'',
and which \textit{temporarily} rejects messages that come from senders
who are not in the black list or in the white list, so that the system
does not know if it is a spam message or not. And, like in our
approach, it works trying to have a minimal impact on the users.

Finally, a system named MUSES (from Multiplatform Usable Endpoint
Security System) \cite{MUSES_SAC_14} is being developed under the
European Seventh Framework  programme (FP7). This system will include
event treatment on the user actions inside a company, DM techniques
for applying the set of policies from the company ISP to the actions,
allowing or denying them, CI techniques for enhancing the system
performance, and ML techniques for improving the set of rules derived
from these policies, according to user's feedback and behaviour after
the system decisions \cite{muses_sotics_13}. The results of this work
could be applied in this system, by changing the pre-processing step,
due to the fact that the database is different. But overall, our
conclusions can be escalated to be included in such a system.

In the next Section, we will describe the problem we aim to solve, in addition to the data from which the data sets are composed.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%% PROBLEM DESCRIPTION %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%

\section{Problem and Data Description}
\label{sec:problemDescription}

\noindent The problem to solve is related with the application of corporate security policies in order to deal with potential URL accesses inside an enterprise. To this end a dataset of URL sessions (requests and accesses) is analysed. These data are labelled with the corresponding permission or denial for that access, following a set of rules. The rules themselves act as a mix between a black list and a white list. The problem is then transformed into a classification one, in which every new URL request will be classified, and thus, a grant or deny action will be assigned to that pattern.

The analysed data come from an \texttt{access.log} of the Squid proxy application \cite{squid:site}, in a real Spanish company. This open source tool works as a proxy, but with the advantage of storing a cache of recent transactions so future requests may be answered without asking the origin server again \cite{DuaneWessels2004}.

Every pattern, namely an URL request, has ten associated variables. These patterns are described in Table \ref{tabdata} in which we have indicated the type of  each variable; either if it is numeric or nominal/categorical. The table has, however, not only ten but eleven described variables. This is due to the fact that we decided to consider the `Content Type' of the requested web page as a whole, but also its Main Content Type (MCT) separately. By adding more information through a new feature, we intended to see if more general rules could be obtained by the classifiers, given that there are less possible values for an MCT than for a whole `Content Type'.



\begin{table*}[htpb]
\centering
 \caption{\label{tabdata} Independent Variables corresponding to a URL
   request through \textit{HTTP}. The URLs are parsed as detailed in
   Subsection \ref{sec:buldingdataset}.}

{\scriptsize
\begin{tabular}{llll}
\hline\noalign{\smallskip}
Variable name & Description & Type & Rank\\
\noalign{\smallskip}\hline\noalign{\smallskip}
\texttt{http\_reply\_code} & Status of the server response & Categorical & 20 values\\
\texttt{http\_method} & Desired action to be performed & Categorical & 6 values\\
\texttt{duration\_milliseconds} & Session duration & Numerical & integer in [0,357170]\\
\texttt{content\_type} & Media type of the entity-body sent to the recipient & Categorical & 85 values\\
\texttt{content\_type\_MCT} & \textbf{M}ain \textbf{C}ontent \textbf{T}ype of the media type. & Categorical & 11 values\\
\texttt{server\_or\_cache\_address} & IP address & Categorical & 2343 values\\
\texttt{time} & connection hour (in the day) & Date & 00:00:00 to 23:59:59\\
\texttt{squid\_hierarchy} & It indicates how the next-hop cache was selected & Categorical & 3 values\\
\texttt{bytes} & Number of transferred bytes during the session & Numerical & integer in [0,85135242]\\
\texttt{client\_address} & IP address & Categorical & 105 values\\
\texttt{URL} & Core domain of the URL, not taking TLD into account & Categorical & 976 values\\
\noalign{\smallskip}\hline
\end{tabular}
}
\end{table*}

The dependent variable or class is a label which inherently assigns a decision (and so the following action) to every request. This can be: \textit{ALLOW} if the access is permitted according to the CSPs, or can be \textit{DENY}, if the connection is not permitted. These patterns are labelled using an `engine' based in a set of security rules, that specify the decision to make. This process is described in Subsection \ref{sec:buldingdataset}.

These data were gathered along a period of two hours, from 8.30 to 10.30 am (30 minutes after the work started), monitoring the activity of all the employees in a medium-size Spanish company (80-100 people), obtaining 100000 patterns. We consider this dataset as quite complete because it contains a very diverse amount of connection patterns, going from personal to professional issues. Moreover, results derived from the experiments and which are described in Section \ref{sec:results} show that this quantity of data might be big enough, but a more accurate outcome would be given with, for instance, a 24 hours long log.

Later on, Section \ref{sec:methodology} will describe how the data coming from the proxy log is labelled due to the application of the aforementioned rules, and the result will be an initial URL dataset with 12 features. Then, at this stage and after describing the data, it seems necessary to describe the technique that we have used for the URL data pre-processing. Rough Set Theory for feature selection is depicted in the next Section.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   ROUGH SET FOR FEATURE SELECTION  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{Rough Set Based Approach for Feature Selection}
\label{sec:featureselection}

As previously mentioned, it is important to perform data pre-processing on the the initial URL dataset.  To do so, it seems necessary to think about a technique that can, on the one hand, reduce data dimensionality using information contained within the dataset and, on the other hand, be capable of preserving the meaning of the features. Rough Set Theory (RST) \cite{pawlak2008rough} can be used as such a tool to discover data dependencies and to reduce the number of attributes contained in the URL dataset using the data alone, requiring no additional information \cite{jensen2005semantics}. In this Section, the basic concepts of RST for feature selection are highlighted.

\subsection{Preliminaries of Rough Set Theory}

Data are represented as a table where each row represents an object and where each column represents an attribute
that can be measured for each object. Such table is called an ``Information System" (IS). Formally, an IS can be defined as a pair $IS = (U, A)$ where $U = \{x_1, x_2, \ldots, x_n\}$  is a non-empty, finite set of objects called the \emph{``universe"} and $A = \{a_1, a_2, \ldots, a_k\}$ is a non-empty, finite set of attributes. Each attribute or feature $a \in A $ is associated with a set $V_a$ of its value, called the \emph{domain} of $a$. We may partition the attribute set $A$ into two subsets $C$ and $D$, called   \emph{condition} and \emph{decision} attributes, respectively \cite{pawlak2008rough}.


Let $P \subset A$ be a subset of attributes. The indiscernibility relation, denoted by $IND(P)$, is an equivalence relation defined as: $IND(P) =  \{(x, y) \in U \times U \mbox{ : } \forall a \in P, a(x) = a(y)\},$ where $a(x)$ denotes the value of feature $a$ of object $x$. If $(x, y) \in IND(P)$, $x$ and $y$ are said to be \emph{indiscernible} with respect to $P$. The family of all equivalence classes of $IND(P)$ (Partition of $U$ determined by $P$) is denoted by $U/IND(P)$. Each element
in $U/IND(P)$ is a set of indiscernible objects with respect to $P$. Equivalence classes $U/IND(C)$ and $U/IND(D)$
are called \emph{condition} and \emph{decision} classes.


For any concept $X \subseteq U$ and attribute subset $R \subseteq A$, $X$ could be approximated by the R-\emph{lower} approximation and R-\emph{upper} approximation using the knowledge of $R$. The lower approximation of $X$ is the set of objects of $U$ that are surely in $X$, defined as: $ \underline{R}(X) = \bigcup \{E \in U/IND(R)  : E \subseteq X \}.$ The upper approximation of $X$ is the set of objects of $U$ that are possibly in $X$, defined as: $\overline{R}(X) = \bigcup \{E \in U/IND(R)  : E \cap X \ne \emptyset \}$. The boundary region is defined as:

\begin{displaymath}
BND_R(X) = \overline{R}(X) - \underline{R}(X)
\end{displaymath}

If the boundary region is empty, that is, $\overline{R}(X) = \underline{R}(X)$, concept $X$ is said to be R-\emph{definable}. Otherwise $X$ is a rough set with respect to $R$.

The positive region of decision classes $U/IND(D)$ with respect to condition attributes $C$ is denoted by $POS_c(D)$ where:

\begin{displaymath}
POS_c(D) = \bigcup \overline{R}(X)
\end{displaymath}

The positive region $POS_c(D)$ is a set of objects  of $U$ that can be classified with certainty to classes $U/IND(D)$ employing attributes of $C$. In other words, the positive region $POS_c(D)$ indicates the union of all the equivalence classes defined by $IND(P)$ that each for sure can induce the decision class $D$.

\subsection{Reduction Process}
\label{subsec:quickreduct}

The aim of feature selection is to remove unnecessary features to the target concept. It is the process of finding a smaller set of attributes, than the original one, with the same or close classification power as the original set. Unnecessary features, in an information system, can be classified into irrelevant features  that do not affect the target concept in any way  and redundant (superfluous) features that do not add anything new to the target concept.

RST for feature selection is based on the concept of  discovering dependencies between attributes. Intuitively, a set of attributes $Q$ depends totally on a set of attributes $P$, denoted $P \rightarrow Q$, if all attribute values from $Q$ can be uniquely determined by values of attributes from $P$. In particular, if there exists a functional dependency between values of $Q$ and $P$, then $Q$ depends totally on $P$. Dependency can be defined in the following way: For $P$, $Q  \subset A$, $Q$  depends on $P$ in a degree $k$ $(0 \leq k \leq 1)$,  denoted $P$ $\rightarrow_{k}$ $Q$, if $k = \gamma_{P}(Q) = |POS_{P}(Q)|/|U|$;  If k = 1 $Q$ depends totally on $P$, if $k < 1 $  $Q$ depends partially (in a degree k) on $P$, and if k = 0 $Q$ does not depend on $P$.

RST performs the reduction of attributes by comparing equivalence relations generated by sets of attributes. Attributes are removed so that the reduced set, termed ``Reduct", provides the same quality of classification as the original. A \emph{reduct} is defined as a subset $R$ of the conditional attribute set $C$ such that $\gamma_{R}(D) = \gamma_{C}(D)$. Thus, a given data set may have many attribute reduct sets. In RST, a \emph{reduct} with minimum cardinality is searched for; in other words an attempt is made to locate a single element of the minimal reduct set. A basic way of achieving this is to generate all possible subsets and retrieve those with a maximum rough set dependency degree. However, this is an expensive solution to the problem and is only practical for very simple data sets. Most of the time, only one reduct is required as, typically, only one subset of features is used to reduce a data set, so all the calculations involved in discovering the rest are pointless. Another shortcoming of finding all possible reducts using rough sets is to inquire about which is the best reduct for the classification process.  The solution to these issues is to apply a \emph{``heuristic attribute selection"} method \cite{zhong2001using}.

Among the most interesting heuristic methods proposed in literature, we mention the \emph{``QuickReduct"} algorithm \cite{shen2007rough} presented by Algorithm \ref{QuickReduct}.

\begin{algorithm}[!ht]
\caption{The QuickReduct Algorithm} \label{QuickReduct}
\begin{algorithmic}[1]
\STATE C: the set of all conditional features;\\
\STATE D: the set of decision features;\\
\STATE R $\leftarrow$ \{\};
\STATE \textbf{do} \\
\STATE \hspace{0.24cm} T $\leftarrow$ R\\
 \STATE \hspace{0.24cm} $\forall$ x $\in$ (C - R);\\
 \STATE \hspace{0.28cm} \textbf{if} $\gamma_{R \cup \{x\}}(D) > \gamma_T(D)$;\\
  \STATE \hspace{0.35cm} T $\leftarrow$ R $\cup$ \{x\};\\
  \STATE \hspace{0.28cm} \textbf{end if} \\
  \STATE \hspace{0.24cm} R $\leftarrow$ T;\\
  \STATE \textbf{until} $\gamma_R(D) == \gamma_C(D)$\\
  \STATE \textbf{return} R
\end{algorithmic}
\end{algorithm}

The \emph{``QuickReduct"} algorithm  attempts to calculate a reduct without exhaustively generating all possible subsets. It starts off with an empty set and adds in turn, one at a time, those attributes that result in the greatest increase in the rough set dependency metric. According to the QuickReduct algorithm, the dependency of each attribute is calculated and the best candidate is chosen. This process continues until the dependency of the reduct equals the consistency of the data set.
For further details about how to compute a  reduct using the   QuickReduct algorithm, we kindly invite the reader to refer to  \cite{shen2007rough}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   METHODOLOGY  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%

\section{Followed Methodology}
\label{sec:methodology}

\noindent Before classification techniques are applied, a data
pre-processing step has been performed. First, the raw dataset is
labelled according a set of \textit{initial corporate security rules},
i.e. every pattern is assigned to a label indication if the
corresponding URL request/access would be ALLOWED or DENIED
considering these rules. This step is necessary in order to transform
the problem into a classification one. However, in order to apply the
rules they must be transformed from their initial format into another
one that can be applied in our programs, a hash or map. This is
described in Subsection
\ref{sec:buldingdataset}. This Subsection also details how the
patterns of the navigation data log (URL sessions) are parsed, in
order to build a hash to perform the matting/labelling process.

At the end of the `parsing' phase, the two hashes are compared in order to obtain which entries of the log should be ALLOW or DENY, known as the \textit{labelling} step. This is similar to perform a decision process in a security system. This step results in that there are 38972 pattern belonging to class ALLOW (positive class) and 18530 of class DENY (negative class), so just a 67.78\% of the samples belong to the majority class. This represents a very important problem, since a classifier that is trained considering these proportions is supposed to classify all the samples as ALLOW, getting a theoretically quite good classification accuracy equal or greater than 68\%. However, in Section \ref{sec:results} we will see that, despite the fact that some denied patterns are classified as allow, the overall performance of the classifiers is better than expected.

It is worth to mention that there is not the same amount of patterns in the two classes. This means that the dataset is unbalanced, and therefore Subsection \ref{subsec:balancing} describes the balancing techniques used for dealing with this situation. Finally, in Subsection \ref{subsec:preprocessing} we explain the applied methods in the pre-processing phase. What we want to prove is that by adding this phase, it enhances the results of our previous work presented in \cite{ECTA}.

Based on the generated pre-processed and balanced dataset and as a final step, a supervised classification process \cite{classification_67} has been conducted. For this step, Weka Data Mining Software\cite{weka:site} has been used, in order to select the best set of classifiers in order to deal with these data. These classifiers will be further tested in Section \ref{sec:results}.

% ------------------------------------------------------------------
%
\subsection{Building the dataset}
\label{sec:buldingdataset}

In previous sections, it was stated that the data to work with was not originally presented in the form of a dataset. Instead, `raw' data was gathered. In order to have the data in the form of a dataset, ready to be pre-processed, as well as being adequate to act as an input for the classifiers, a parsing process must be performed.

First, in this work we have considered Drools \cite{drools:site} as the tool to create and  manage rules in a business environment. This so called Business Rule Management System (BRMS) has been developed by the JBoss community under an Apache License and it is written in Java. Though this platform consists of many components; here we focus on Drools Expert and the Drools Rule Language (DRL, \cite{drools:doc}). Then, the defined rules for a certain company are included in a file with a \texttt{.drl} extension; the file that needs to be parsed to obtain the final set of rules. To obtain the needed knowledge from the rules file, it is necessary to know the format of this type of language, because it is essential for the parsing process.

In Figure \ref{fig:drools_hash}, (a), we display the typical rule syntax in DRL. Two main parts should be obtained from the parsing method that will be applied: both left and right sides of the rule, taking into account that the left side is where the company specifies the conditions required to apply the action indicated in the right side. Also, for describing the conditions, Squid syntax is used (see Section \ref{sec:problemDescription}), having thus the following structure: \texttt{squid:Squid(\textit{conditions})}. Finally, from the right side of the rule, the \textit{ALLOW} or \textit{DENY} label to apply on the data which matches with the conditions, will be extracted.
The parser that we have implemented applies two regular
expressions, one for each side of the rule, and returns a hash with
all the rules with the conditions and actions defined. The `before and
after' performing the parsing over the \texttt{.drl} file is presented
in Figure \ref{fig:drools_hash}.

\begin{figure}[htb]
\centering
\subfloat[Drools Rule]{
\begin{tabular}{ p{0.5cm} p{0.5cm} p{4cm} }
  \texttt{rule~"name"} & & \\
   & \texttt{attributes} & \\
   & \texttt{when} & \\
   & & \texttt{/* Left Side of the Rule */} \\
   & \texttt{then} & \\
   & & \texttt{/* Right Side of the Rule */} \\
  \texttt{end} & & \\
\end{tabular}
}
~
\subfloat[Hash Rule]{
\begin{tabular}{ p{0.5cm} p{0.5cm} p{4cm} }
  \texttt{\%rules~=~(} & & \\
   & \texttt{rule~=>\{} & \\
   & & \texttt{field => xxx} \\
   & & \texttt{relation => xxx} \\
   & & \texttt{value => xxx} \\
   & & \texttt{action => [allow, deny]} \\
   & \texttt{\},} & \\
  \texttt{);} & & \\
\end{tabular}
}
\caption{(a) Structure of a rule in Drools Expert. (b) Resulting rule, after the parsing, in a global hash of rules. \label{fig:drools_hash}}
\end{figure}

Then, the log file is analysed. Usually, the instances of a log file have a number of fields (which will be later referred as features/attributes of a connection pattern), in order to have a registration of the client who asks for a resource, the time of the day when the request is made, and so on. In this case, we have worked with an \textit{access.log} (see Section \ref{sec:problemDescription}) file, converted into a CSV format file so it could be parsed and transformed in another hash of data. Also, all ten fields of the Squid log yield another hash. Once the two hashes of data were created, they were compared in such a way that for each rule in the hash of rules, it was determined how many entries in the data log hash are covered by the rule, and so they were applied the label that appears as `action' in the rule.

Among the tasks to be performed, is the one to extract from a whole URL the part that was more interesting for our defined purposes. It is important to point out that in a log with thousands of entries, an enormous variety of URLs can be found, since some can belong to advertisements, images, videos, or even some others does not have a domain name but are given directly by an IP address. For this reason, we have taken into account that for a domain name, many subdomains (separated by dots) could be considered, and their hierarchy grows from the right towards the left. The highest level of the domain name space is the Top-Level Domain (TLD) at the right-most part of the domain name, divided itself in country code TLDs and generic TLDs. Then, a domain and a number of subdomains follow the TLD (again, from right to left). In this way, the URLs in the
used log are such as \textit{http://subdomain...subdomain.domain.TLD/} \textit{other\_subdirectories}. However, for the ARFF\footnote{Format of Weka files} file to be created, only the domain (without the subdomains and the TLD) should be considered, because there are too many different URLs to take into consideration. Hence, applying another regular expression, the data parser obtains all the core domains of the URLs, which makes 976 domains in total.

% ------------------------------------------------------------------
%
\subsection{Balancing the dataset}
\label{subsec:balancing}

While analysing the data, we observed that more than half of the
initial amount of patterns are labelled, and that the ratio is 2:1 in
allows to denies. The 2:1 ratio means that the data is unbalanced, and
therefore we have performed different approaches from the first and
second groups of data balancing techniques, which were introduced in
Section \ref{subsec:dataanalysis}:

\begin{itemize}
\item Undersampling: we will randomly remove samples of the majority class until the amount in both classes are similar. In other words, we will reduce the amount of `denied' patterns by a half.
\item Oversampling: we will introduce more samples in the minority class, in order to get a closer number of patterns in both classes. This has to be done due to the impossibility of creating synthetic data when dealing with categorical values, given that there is not a proper distance measure between two values in a category. Actually, since the number of samples in the majority class is almost twice the minority one, we have just duplicated all of those belonging to the minority class.
\end{itemize}

% ------------------------------------------------------------------
%
\subsection{Pre-processing the data}
\label{subsec:preprocessing}

Section \ref{subsec:dataanalysis} explains that having a good quality database is crucial when good accuracy values are required. For this reason, we have maintained the dataset by performing a removal of patterns in the log which we found as redundant and applied a feature selection technique, based on Rough Sets.

% ------------------------------------------------------------------
%
\subsubsection{Erasing redundant information}
\label{subsec:duplicateddata}

The first thing to do is studying the data in order to look for the patterns that are repeated. Hence, after having analysed the log of connecting patterns, we studied the field \textit{squid\_hierarchy} and saw that had two possible values: \texttt{DIRECT} or \texttt{DEFAULT\_PARENT}. The Squid FAQ reference \cite{squid_logs}, and the Squid wiki \cite{squid_wiki} explain that, as a proxy, the connections are made, firstly to the Squid proxy, and then, if appropriate, the request continues to another server. These connections are registered in Squid in the same way, with the same fields, with the exception of the client and server IP addresses. From the point of view of classification, if one of these two entries happens to be in the training file, and the other in the testing file, it would mean that the second would be correctly classified because of all the attribute values that both have in common. However, this also means that the good percentages that we obtained may not be real, but biased. That is why the second step is about removing entries that we called ``repeated'' (in the explained sense). This step is performed over the original, unbalanced, dataset. After the removal, a new file was created.

% ------------------------------------------------------------------
%
\subsubsection{Performing feature selection}
\label{subsec:featselresults}

For pattern classification, our learning problem has to select high discriminating features from the input database which corresponds to the URL information dataset. To perform this task, we apply rough set theory.

Technically, we may formalize our problem as an information system where universe $U = \{x_1, x_2, \ldots, x_N\}$ is a set of pattern identifiers, the conditional attribute set $C =\{c_1,c_2, \ldots, c_N\}$ contains each feature of the information table to select and the decision attribute $D$ of our learning problem corresponds to the class label of each pattern. The input database has a single binary decision attribute. Hence, the decision attribute $D$  has binary values $d$:  either the HTTP request is allowed or denied. The condition attribute feature $D$ is defined as follows:
\begin{displaymath}
D =\{Allow, Deny\}
\end{displaymath}

For feature selection, we apply the rough \emph{``QuickReduct"} algorithm which was previously explained in Section \ref{subsec:quickreduct} . First of all, the dependency of the entire database $\gamma_{C}(D)$ is calculated. To do so, the algorithm has to calculate the positive region for the whole attribute set $C$: $POS_{C}(D)$. Once the consistency of the database is measured, the feature selection process starts off with an empty set and moves to calculate the dependency of each attribute $c$ apart: $\gamma_{c}(D)$. The attribute $c$ having the greatest value of dependency is added to the empty set. Once the first attribute $c$ is selected, the algorithm adds, in turn, one attribute to the selected first attribute and computes the dependency of each obtained attributes couple $\gamma_{\{c, c_{i}\}}(D)$. The algorithm chooses the couple having the greatest dependency degree. The process of adding each time one attribute to the subset of the selected features continues until the dependency of the obtained subset equals the consistency of the entire database already calculated; i.e., $\gamma_{C}(D)$.

From the initial dataset containing 12 features and after applying the rough feature selection technique, we obtained a list of 9 features. The features kept after the feature selection process are the following:

\begin{itemize}
  \item http\_reply\_code
  \item duration\_miliseconds
  \item content\_type
  \item server\_or\_cache\_address
  \item time
  \item bytes
  \item url
  \item client\_address
\end{itemize}

On the contrary, the following features were erased by applying Rough Set for feature selection:

\begin{itemize}
  \item http\_method
  \item content\_type\_MCT
  \item squid\_hierarchy
\end{itemize}

In Section \ref{subsec:RSTresults}, we will show that by applying rough set theory for selecting the most important features is a good way of maintaining the good quality of the database, and the system performance will improve significantly.

% ------------------------------------------------------------------
%
\subsection{Classification Methods}
\label{subsec:methods}

\noindent The choice of the classifiers to apply and would be admitted
before we make a test selection phase -- known as a `pre-selection
phase' -- is based on two main requirements. First and as our goal
consists of obtaining a set of rules able to classify unknown URL
connection requests,  we need classifiers based on decision trees or
rules, so that we can study their output in addition to their
accuracy. Second and as mentioned in Section
\ref{sec:problemDescription}, the features of the data used for this
work are mainly categorical, but also numerical. Thus, among the
classifiers based on trees or rules, we need classifiers that are able
to handle these types of features. Consequently, we made a first
selection over all the classifiers in Weka which complied with these
requirements.

Yet, it is important to mention that previously in \cite{ECTA}, we studied a set of classifiers that may be applied to the nature of our dataset; classifiers that fit our requirements. The selected classifiers are indeed based on the obtained good classification accuracy. In fact, we demonstrated that the classifiers that lead to better classification results were:

\begin{description}


 \item[J48] This classifier generates a pruned or unpruned C4.5 decision tree. Described for the first time in 1993 by \cite{Quinlan1993}, this machine learning method builds a decision tree selecting, for each node, the best attribute for splitting and create the next nodes. An attribute is selected as `the best' by evaluating the difference in entropy (information gain) resulting from choosing that attribute for splitting the data. In this way, the tree continues to grow till there are not attributes anymore for further splitting, meaning that the resulting nodes are instances of single classes.


\item[Random Forest] This manner of building a decision tree can be seen as a randomization of the previous C4.5 process. It was stated by \cite{Breiman2001} and consist of, instead of choosing `the best' attribute, the algorithm randomly chooses one between a group of attributes from the top ones. The size of this group is customizable in Weka.

\item[REP Tree] Is another kind of decision tree, it means Reduced Error Pruning Tree. Originally stated by \cite{Quinlan1987}, this method builds a decision tree using information gain, like C4.5, and then prunes it using reduced-error pruning. That means that the training dataset is divided into  two parts: one devoted to make the tree grow and another for pruning. For every subtree (not a class/leaf) in the tree, it is replaced by the best possible leaf in the pruning three and then it is tested with the test dataset if the made prune has improved the results. A deep analysis about this technique and its variants can be found in \cite{Elomaa2001}.

\item[NNge] Nearest-Neighbor machine learning method of generating rules using non-nested generalised exemplars, i.e. the so called `hyperrectangles' for being multidimensional rectangular regions of attribute space \cite{Martin1995}. The NNge algorithm builds a ruleset from the creation of this hyperrectangles. They are non-nested (overlapping is not permitted), which means that the algorithm checks, when a proposed new hyperrectangle created from a new generalisation, if it has conflicts with any region of the attribute space. This is done in order to avoid that an example is covered by more than one rule (two or more).

 \item[PART] It comes from `partial' decision trees, for it builds its rule set from them \cite{Frank1998}. The way of generating a partial decision tree is a combination of the two aforementioned strategies ``divide-and-conquer'' and ``separate-and-conquer'', gaining then flexibility and speed. When a tree begins to grow, the node with lowest information gain is the chosen one for starting to expand. When a subtree is complete (it has reached its leaves), its substitution by a single leaf is considered. At the end the algorithm obtains a partial decision tree instead of a fully explored one, because the leafs with largest coverage become rules and some subtrees are thus discarded.
 \end{description}

These methods will be deeply tested on the dataset (balanced and unbalanced) in the following section.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   RESULTS  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{Results}
\label{sec:results}

This section presents the obtained results for the different configurations of the experimental setup. First, Subsection \ref{subsec:firstresults} depicts the first results, summarising what was proved in \cite{ECTA}. This means that, for the chosen classifiers, described in Section \ref{subsec:methods}, results are displayed for the dataset when it is in its initial - unbalanced - form, as well as after the balancing process. At the end of this Subsection, we introduce the results obtained once the dataset is released from the redundant patterns. Then, Subsection \ref{subsec:RSTresults} presented the results obtained when applying rough set theory as a feature selection technique to the balanced generated dataset. This subsection justifies that the use of rough sets  enhances the system performance in terms of  both execution/running time and classification accuracy. Finally, examples of the obtained rules which were taken from the classifiers' output, are discussed in Subsection \ref{subsec:rulesdiscussion}.

\subsection{Results about classification}
\label{subsec:firstresults}

\noindent Several experiments have been conducted, once a subset of classification methods has been chosen (see Section \ref{subsec:methods}). In order to better test the methods, two different divisions (training-test) have been done; namely 90\%-10\% and 80\%-20\%. Also, it is worth mentioning that we have included Na\"ive Bayes in the result tables, as it is normally used as a reference classifier in classification problems \cite{Frank2011}.

Moreover, the way in which those divisions were built has been considered as: randomly built, or sequentially built. We say that the training and test files were randomly built when the patterns are taken from the original dataset and, by generating a random number, they have a certain probability to belong to the training file, and another to belong to the test file. On the contrary, the training and test files are built sequentially when the patterns inside them strictly follow the same order in time as the original dataset, before being divided. The aim of the sequential division is to compare if the online activity of the employees, considering URL sessions, could be somehow ``predicted'', just using data from previous minutes or hours.

As stated in Section \ref{subsec:balancing}, the dataset presents unbalance in the data due to the fact that there are more patterns classified as `allow' than `deny'. Therefore, two data balancing methods have been applied to all the files to get similar pattern amounts in both classes: undersampling (random removal of ALLOW patterns) and oversampling (duplication of DENY patterns).

Classification results for the unbalanced data are presented in Table \ref{tabresults_nobalan}. Three different tests have been performed for the random pattern distribution approach, so the mean and standard deviation are shown in the corresponding columns.



\begin{table*}[htpb]
\centering
 \caption{\label{tabresults_nobalan} Percentage of correctly classified patterns for the unbalanced dataset with 12 features.}
{\small
\begin{tabular}{|l|c|c|c|c|}
\cline{2-5}
\multicolumn{1}{l|}{} & \multicolumn{2}{c|}{80\% Training - 20\% Test} & \multicolumn{2}{c|}{90\% Training - 10\% Test} \\
\cline{2-5}
\multicolumn{1}{l|}{} & Random (mean) & Sequential & Random (mean) & Sequential \\
\hline
Na\"ive Bayes & 91.60 $\pm$ 1.25 & 85.53 & 92.89 $\pm$ 0.12 & 83.84 \\
\cline{1-1}
J48 & 97.56 $\pm$ 0.20 & 88.48 & 97.70 $\pm$ 0.15 & 82.28 \\
\cline{1-1}
Random Forest & 97.68 $\pm$ 0.20 & 89.77 & 97.63 $\pm$ 0.13 & 82.59 \\
\cline{1-1}
REP Tree & 97.47 $\pm$ 0.11 & 88.34 & 97.57 $\pm$ 0.01 & 83.20 \\
\cline{1-1}
NNge & 97.23 $\pm$ 0.10 & 84.41 & 97.38 $\pm$ 0.36 & 80.34 \\
\cline{1-1}
PART & 97.06 $\pm$ 0.19 & 89.11 & 97.40 $\pm$ 0.16 & 84.17 \\
\hline
\end{tabular}
}
\end{table*}

As it can be seen from Table \ref{tabresults_nobalan}, all five
classifiers achieved a high performance classifying in the right way
the test dataset. Also, having low values of standard deviation means
that the obtained accuracies are stable; and this can be seen from the
obtained results as well.

For a 80\%-20\% division, results based on the sequential data have lower values than those obtained from the random data, but still they are considered as good ($>$ 85\%). This is due to the occurrence of new patterns from a certain time. Some requests may happen just at one specific time of the day, or in settled days. Then, the classifier may not find enough similarity in the patterns to correctly classify the entries in the test file. On the other hand, the loss of 5 to 6 points in the results of the 90\%-10\% division is somehow expected as it reinforces the previous mentioned hypothesis.

The classifier that lightly stands out over the others is \textit{Random Forest}, being the best in almost every case for randomly made divisions, and it also has good results for sequentially made divisions. However, if we focus on the standard deviation, \textit{REP Tree} is the chosen
one, as its results present robustness.

Once balancing is performed, resulting datasets were used as inputs for the same classifiers, and results are shown in Tables \ref{tabresults_balan_under} and \ref{tabresults_balan_over}. Table \ref{tabresults_balan_under} shows the classifiers' accuracy for the balanced dataset with 12 features, applying undersampling technique, and Table \ref{tabresults_balan_over}, with the application of oversampling technique. For each one, the 90\%-10\% and 80\%-20\% divisions were also made.

\begin{table*}[htpb]
\centering
 \caption{\label{tabresults_balan_under} Percentage of correctly classified patterns for the balanced dataset with 12 features, applying undersampling technique.}
{\small
\begin{tabular}{|l|c|c|c|c|}
\cline{2-5}
\multicolumn{1}{l|}{} & \multicolumn{2}{c|}{80\% Training - 20\% Test} & \multicolumn{2}{c|}{90\% Training - 10\% Test} \\
\cline{2-5}
\multicolumn{1}{l|}{} & Random (mean) & Sequential & Random (mean) & Sequential \\
\hline
Na\"ive Bayes & 91.30 $\pm$ 0.20 & 84.94 & 91.74 $\pm$ 0.13 & 85.43 \\
\cline{1-1}
J48 & 97.05 $\pm$ 0.25 & 84.29 & 96.85 $\pm$ 0.35 & 76.44 \\
\cline{1-1}
Random Forest & 96.61 $\pm$ 0.17 & 88.59 & 96.99 $\pm$ 0.13 & 79.98 \\
\cline{1-1}
REP Tree & 96.52 $\pm$ 0.13 & 85.54 & 96.55 $\pm$ 0.10 & 77.65 \\
\cline{1-1}
NNge & 96.56 $\pm$ 0.42 & 85.28 & 96.33 $\pm$ 0.05 & 81.93 \\
\cline{1-1}
PART & 96.19 $\pm$ 0.14 & 85.16 & 96.09 $\pm$ 0.10 & 79.70 \\
\hline
\end{tabular}
}
\end{table*}

\begin{table*}[htpb]
\centering
 \caption{\label{tabresults_balan_over} Percentage of correctly classified patterns for the balanced dataset with 12 features, applying oversampling technique.}
{\small
\begin{tabular}{|l|c|c|c|c|}
\cline{2-5}
\multicolumn{1}{l|}{} & \multicolumn{2}{c|}{80\% Training - 20\% Test} & \multicolumn{2}{c|}{90\% Training - 10\% Test} \\
\cline{2-5}
\multicolumn{1}{l|}{} & Random (mean) & Sequential & Random (mean) & Sequential \\
\hline
Na\"ive Bayes & 91.18 $\pm$ 0.16 & 82.35 & 91.77 $\pm$ 0.28 & 81.81 \\
\cline{1-1}
J48 & 97.40 $\pm$ 0.03 & 85.66 & 97.37 $\pm$ 0.06 & 74.24 \\
\cline{1-1}
Random Forest & 97.16 $\pm$ 0.19 & 89.03 & 97.25 $\pm$ 0.33 & 81.33 \\
\cline{1-1}
REP Tree & 97.13 $\pm$ 0.25 & 85.41 & 97.14 $\pm$ 0.09 & 76.81 \\
\cline{1-1}
NNge & 96.90 $\pm$ 0.28 & 83.46 & 96.91 $\pm$ 0.06 & 78.73 \\
\cline{1-1}
PART & 96.82 $\pm$ 0.09 & 84.50 & 96.68 $\pm$ 0.11 & 78.16 \\
\hline
\end{tabular}
}
\end{table*}

\begin{description}
  \item[Applying Undersampling] In comparison with those results from Table \ref{tabresults_nobalan}, these go down one point (in the case of randomly made divisions) to six points (sequential divisions). The reason why this happens is that when randomly removing ALLOW patterns, we really are losing information, i. e. key patterns that could be decisive in a good classification of a certain set of test patterns.
  \item[Applying Oversampling] Here we have duplicated the DENY patterns so their number could be up to that of the ALLOW patterns. However, it does not work as well as in other approaches which uses numerical computations for creating the new patterns to include in the minority class. Consequently, the results have been decreased.
\end{description}

In both cases, it is noticeable that if we take the data in a sequential way, instead of randomly, results will decrease. Also, it is clear that due to the fact that performing undersampling some patterns are lost while in the case of oversampling they all remain, and this leads to have better results with the \textit{oversampling} balancing technique. Then, in this case the algorithm with best performance is \textit{J48}, though \textit{Random Forest} follows its results very closely in random datasets processing, and \textit{REP Tree}, which is better than the rest when working with sequential data. Nevertheless, generally speaking and given the aforementioned reasons, performing data balancing methods decreases  the results.

Once this first study is finished, the next step is to erase the duplicated requests. And then, we test the obtained reduced dataset to see if it has some influence on the results. As it seems that the best results are obtained for an unbalanced dataset, and also for a training-test random division, we choose this configuration for the following experiments.

The results are displayed in Table \ref{tab:repurl_unb_traintest}. We can see that the results slightly decrease in comparison to the ones obtained originally, but they are still good, and definitely better than Na\"ive Bayes.

The way it happened for the original datasets, results for files with the patterns taken consecutively lower significantly. And as previously explained, this happens due to the possible loss of information. Best results are obtained by both \textit{Random Forest} and \textit{REP Tree} classifiers, with a 96\% of accuracy.

\begin{table*}[htpb]
\centering
 \caption{\label{tab:repurl_unb_traintest}Percentage of correctly classified patterns for unbalanced data, after the removal of entries that could lead to misclassification.}
{\small
\begin{tabular}{|l|c|c|c|c|}
\cline{2-5}
\multicolumn{1}{l|}{} & \multicolumn{2}{c|}{80\% Training - 20\% Test} & \multicolumn{2}{c|}{90\% Training - 10\% Test} \\
\cline{2-5}
\multicolumn{1}{l|}{} & Random (mean) & Sequential & Random (mean) & Sequential \\
\hline
Na\"ive Bayes & 93.01 $\pm$ 0.32 & 82.61 & 93.09 $\pm$ 0.91 & 83.04 \\
\cline{1-1}
Random Forest & 96.97 $\pm$ 0.47 & 91.03 & 96.79 $\pm$ 0.97 & 80.60 \\
\cline{1-1}
J48 & 96.90 $\pm$ 0.26 & 87.78 & 96.50 $\pm$ 1.00 & 84.49 \\
\cline{1-1}
NNge & 96.21 $\pm$ 0.28 & 81.17 & 96.11 $\pm$ 1.13 & 81.92 \\
\cline{1-1}
REP Tree & 96.97 $\pm$ 0.40 & 87.75 & 96.62 $\pm$ 0.87 & 85.57 \\
\cline{1-1}
PART & 96.84 $\pm$ 0.18 & 86.68 & 96.55 $\pm$ 0.87 & 83.61 \\
\hline
\end{tabular}
}
\end{table*}

First, we concluded that not balancing the dataset was better for obtaining good results, also that taking the samples randomly instead of a sequential way is more adequate. Finally, we noticed that we have successfully reduced the dataset and did not lose good accuracies. For this reason, the dataset with the redundant patterns erased is the chosen one to perform the feature selection. Results are described in next subsection.


\subsection{Results about feature selection}
\label{subsec:RSTresults}

In this section, our aim is to prove two hypotheses. First, we want to
prove that applying rough set theory for feature selection reduces the
running time when testing the classifiers. Second and based on the
reduced feature set of data, we want to prove that the accuracies
remain the same, or even improve in comparison to the original set of
features.

The resulting reduced dataset, from the previous subsection, was used to test the same chosen classifiers, plus JRip. This is a classifier which consists of a propositional rule learner, the so-called Repeated Incremental Pruning to Produce Error Reduction (RIPPER) algorithm. It was proposed in \cite{cohen1995fast} as an improved version of the Incremental Reduced Error Pruning (IREP) algorithm. The reason why this JRip classifier was added to the list, is because we cannot compare the size of the trees for the Random Forest classifier, as the size of the forest is chosen when running it. Then, we added JRip for making the comparison more complete.

All the experiments were made with the same computer, in the following
conditions: Toshiba Laptop with Intel\texttrademark  Core i7-3630QM,
CPU@2.40GHz x 8; RAM 3.8 GB; operating system 64 bit Ubuntu Linux 14.04 LTS; and Weka version 3.6.10, with 3 GB assigned for memory
usage. Table \ref{tab_runningtimes} shows the results of the
comparison between performance before and after applying feature
selection. Though the complexity of the trees generated by the
classifiers grows after applying feature selection, even obtaining
more rules for PART classifiers, the running time lowers by an average
of 40\%.


\begin{table}[htpb]
\centering
 \caption{\label{tab_runningtimes} Comparison between rule/tree complexity and running times for the initial data set, which had 12 features, and the resulting one, having 9 features, after applying Rough Set Theory for feature selection.}
{\small
\begin{tabular}{|l|c|c|}
\cline{2-3}
\multicolumn{1}{l|}{} & 12 features & 9 features \\
\hline
J48 & Size of the tree 8113, 0.62 seconds & Size of the tree 10191, 0.26 seconds \\
\cline{1-1}
Random Forest & 10 trees, 1.59 seconds & 10 trees, 1.32 seconds \\
\cline{1-1}
REP Tree & Size of the tree 8317, 2.24 seconds & Size of the tree 8817, 0.53 seconds \\
\cline{1-1}
NNge & 1341 exemplars, 48.11 seconds & 1294 exemplars, 39.57 seconds \\
\cline{1-1}
PART & 966 rules, 15.38 seconds & 998 rules, 11.24 seconds \\
\cline{1-1}
JRip & 87 rules, 127.28 seconds & 64 rules, 64.23 seconds \\
\hline
\end{tabular}
}
\end{table}

Now, if we focus on the results of the accuracies which are summarised in Table \ref{tab_12featvs9feat}, we notice that the classification accuracies are nearly the same. This comparison was made in the same conditions which are unbalanced datasets, and with a 10 fold cross-validation technique for training-test.

\begin{table}[htpb]
\centering
 \caption{\label{tab_12featvs9feat} Comparison between the obtained accuracies for the initial data set, which had 12 features, and the resulting one, having 9 features, after applying Rough Set Theory for feature selection.}
{\small
\begin{tabular}{|l|c|c|}
\cline{2-3}
\multicolumn{1}{l|}{} & 12 features & 9 features \\
\hline
Na\"ive Bayes & 92.30 & 92.19 \\
\cline{1-1}
J48 & 97.37 & 97.40 \\
\cline{1-1}
Random Forest & 97.60 & 97.57 \\
\cline{1-1}
REP Tree & 97.33 & 97.34 \\
\cline{1-1}
NNge & 97.18 & 97.13 \\
\cline{1-1}
PART & 97.41 & 97.27 \\
\cline{1-1}
JRip & 92.95 & 91.66 \\
\hline
\end{tabular}
}
\end{table}

Moreover, results in Table \ref{tab_featselect} show the same behaviour as for previous experiments. Results from this table are compared to those obtained from Table \ref{tabresults_nobalan}. This is because both tables are sharing nearly the same conditions, except that each one has a specific number of features.

We can see that, for example, for a division of 80\% training - 20\% test, the results after feature selection are better for the \textit{Random Forest} and \textit{PART} classifiers. Only \textit{NNge} seems to generate lower classification results, but still considered as interesting as it is higher than 96\%.

\begin{table*}[htpb]
\centering
 \caption{\label{tab_featselect} Percentage of correctly classified patterns for the unbalanced dataset with 9 features.}
{\small
\begin{tabular}{|l|c|c|c|c|}
\cline{2-5}
\multicolumn{1}{l|}{} & \multicolumn{2}{c|}{80\% Training - 20\% Test} & \multicolumn{2}{c|}{90\% Training - 10\% Test} \\
\cline{2-5}
\multicolumn{1}{l|}{} & Random (mean) & Sequential & Random (mean) & Sequential \\
\hline
J48 & 97.10 $\pm$ 0.23 & 87.83 & 97.33 $\pm$ 0.80 & 84.51 \\
\cline{1-1}
Random Forest & 97.61 $\pm$ 0.49 & 88.06 & 97.76 $\pm$ 0.83 & 83.71 \\
\cline{1-1}
REP Tree & 97.17 $\pm$ 0.15 & 87.79 & 97.39 $\pm$ 0.58 & 85.73 \\
\cline{1-1}
NNge & 96.63 $\pm$ 0.50 & 82.18 & 97.27 $\pm$ 1.12 & 80.94 \\
\cline{1-1}
PART & 97.24 $\pm$ 0.12 & 87.88 & 97.29 $\pm$ 0.86 & 85.11 \\
\hline
\end{tabular}
}
\end{table*}

Finally, we have proved our first hypothesis: applying Rough Set Theory for feature selection significantly improves the computational cost of the system. Also, we proved that our second hypothesis is also true, because the obtained accuracies after applying rough set theory for feature selection are the same, even slightly better, than the ones obtained before the pre-processing phase.

\subsection{Discussion about the Obtained Rules}
\label{subsec:rulesdiscussion}

One of the main objectives of this chapter is to find a method (classifier) that can build rules not dependent on the URL, in order to get a behaviour quite different from the classical black and white lists. Thus, it could made a decision about new connection requests based on other, more general, features.

In the performed experiments, the majority of the obtained rules/trees are based on the URL in order to discriminate between the two classes. However, we also found several ones which consider other variables/features rather than the URL itself to make the decision. For instance:\\

\begin{small}
\begin{verbatim}
IF server_or_cache_address = "173.194.34.225"
AND http_method = "GET"
AND duration_milliseconds > 52
THEN ALLOW

IF server_or_cache_address = "173.194.78.103"
THEN ALLOW

IF content_type =
 "application/vnd.google.safebrowsing-update"
THEN DENY

IF server_or_cache_address = "173.194.78.94"
AND content_type_MCT = "text"
AND content_type = "text/html"
AND http_reply_code = "200"
AND bytes > 772
THEN ALLOW
\end{verbatim}
\end{small}

In their presented format, these rules are considered adequate to fulfill our purposes, since they are somehow independent of the URL to which the client requests to access. Thus, it would be potentially possible to allow or deny the access to unknown URLs just taking into account some parameters of the request.

When the features considered in the rule can be known in advance, such as \texttt{http\_method}, or \texttt{server\_or\_cache\_address}, for instance, the decision could be made in real-time, and thus, a granted URL (whitelisted) could be DENIED, or the other way round.

Tree-based classifiers also yield to several useful branches in this sense, but they have not been plotted here because of the difficulty for showing/visualizing them properly.

Focusing on the presented rules, it can be noticed that almost all of them also depend on very determining features/values, such as \texttt{server\_or\_cache\_address}, or even on the \texttt{client\_address} (not presented here). These features create several non-useful rules, mainly in the case of the client IP address, because it will not be correct to settle that a specific IP cannot access to some URLs. Thus, we have conducted additional experiments in this line by removing these three features (\texttt{url}, \texttt{server\_or\_cache\_address}, \texttt{client\_address}) from the dataset and training again the classifiers. These have been performed over the unbalanced data, considering a 10-fold cross validation test.

The results of classification accuracies are shown in Table \ref{tab_rules_study_classification}.

\begin{table}[htpb]
\centering
 \caption{\label{tab_rules_study_classification} Percentage of correctly classified patterns for the unbalanced dataset without the set of critical features, namely \texttt{URL}, \texttt{server\_or\_cache\_address}, and \texttt{client\_address}.}
{\small
\begin{tabular}{|l|c|c|}
\cline{2-3}
\multicolumn{1}{l|}{} & Without URL feature & Without URL-IPs features\\
\hline
J48 & 93.62 & 90.53 \\
\cline{1-1}
Random Forest & 94.42 & 91.75 \\
\cline{1-1}
REP Tree & 92.58 & 89.61 \\
\cline{1-1}
PART & 93.40 & 88.25 \\
\cline{1-1}
JRip & 87.45 & 85.60 \\
\hline
\end{tabular}
}
\end{table}

As expected, and as it can be seen in Table \ref{tab_rules_study_classification}, the percentages of accuracies have been decreased. Results are more influenced and decrease  in the case where three features have been discarded. However, the results are still quite good, having in mind that the remaining features are more general than these. In addition, it is worth to analyse the set of rules that the classifiers\footnote{Trees can be deployed as rules.} have generated as models. Thus, having a look at these other rules, in the case without \texttt{url} (i.e. 11 features), the rules are pretty similar to those presented before. Thus, we can find, among the most important rules (in the sense that the classification accuracy depends in a big part on them) the following ones:

\begin{small}
\begin{verbatim}
IF bytes >= 1075
AND time >= 29633000
AND time <= 30031000
AND client_address = "10.159.52.182"
AND content_type_MCT = "image"
AND content_type = image/jpeg
THEN DENY

IF server_or_cache_address = "173.194.66.121"
AND client_address = "192.168.4.4"
AND time <= 33603000
THEN ALLOW

IF client_address = "10.159.188.11"
AND bytes <= 2166
AND content_type_MCT = "text"
THEN ALLOW
\end{verbatim}
\end{small}

These rules depend on the so-called `critical features' and almost all the important rules depend on them. Due to this reason, we have performed another experiment omitting these variables, in addition to the \texttt{URL}, as it was mentioned before. The generated rules by some of the classifiers, tree and rule-based, are closer to what we aim to obtain. Some examples of relevant rules are the following:

%[J48], [REP Tree], [PART] and [PART]
\begin{small}
\begin{verbatim}

IF http_reply_code = "200"
AND content_type = "application/json"
AND time <= 33635000
AND bytes <= 3921
THEN ALLOW

IF content_type = "text/plain"
AND duration_milliseconds >= 7233.5
THEN DENY

IF content_type = "application/octet-stream"
AND bytes <= 803
THEN ALLOW

IF bytes <= 1220
AND time <= 33841000
AND http_reply_code = "404"
AND squid_hierarchy = DEFAULT_PARENT
AND duration_milliseconds <= 233
AND bytes <= 722
THEN ALLOW
\end{verbatim}
\end{small}

As it can be seen these are more general rules which could be much more useful for classifying new, previously unknown, URL access requests in a company. These rules could be taken as a reference to build a decision system. However, there are still some considerations that should be taken into account since all the rules have been created using a very specific data. Thus, there are several rules that cannot be used as they are specific for some other companies, and should be supervised somehow (maybe by an expert).

Moreover, some of these features depend on the session itself, i.e. they will be computed after the session is over, but the idea in that case would be `to refine' somehow the existing set of URLs in the white list. Thus, when a client requests access to a whitelisted URL, this will be allowed, but after the session is over and depending on the obtained values, one of these classifiers could label the URL as DENIED for further requests. This could be a useful decision-aid tool for the Chief Security Officer (CSO) inside a company, for instance.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  CONCLUSIONS  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{Conclusions and future work}
\label{sec:conclusions}
In this paper various classification methods have been applied in order to perform a decision process inside a company,
according to some predefined corporate security policies. This decision is focused on allowing or denying URL access
requests by considering previous decisions on similar requests, and not having specific rules in an already defined
white/black list for those URLs. Thus, the proposed method would allow or deny an access to a URL based on additional
features rather than the specific URL string, only. This could be very useful since new URLs could be
automatically 'whitelisted' or 'blacklisted' depending on some of the connection parameters,
such as the \texttt{content\_type} of the access or the \texttt{IP} of the client which makes the request.

To this aim, we have started from a big dataset (100000 patterns) with employees' URL requests information,
and by considering a set of URL access permissions, we have composed a labelled dataset (57000 patterns).
Over that set of data, we have tested several classification methods, after some data balancing techniques
have been applied. Then, the best five classifiers have been deeply proved over several training and test divisions,
and with two methods: by leaving the order in time when the URL were requested, and by taking them in a random way.

The results show that classification accuracies are between 95\% and 97\%, even when using the unbalanced datasets.
However, they have been diminished because of the possible loss of data that comes from performing an undersampling
(removing patterns) method; or taking the training and the datasets in a sequential way from the main log file,
due to the fact that certain URL requests can be made only at a certain time.

After that, we have shown that maintaining the dataset is crucial in order to improve the performance of
the system, mainly, in terms of classification accuracy. We have shown that by erasing the duplicated data,
the accuracies remain inside the range of 96\% - 97\%, which means that indeed there was redundant information
in the dataset.

The resulting dataset was the one over which we have performed feature selection by means of rough sets,
and we have proved that by selecting the most interesting features we could improve the classification accuracy of
the system while being lightweight in terms of running time. In this way, we can conclude that our proposed approach
has been successful and it would be a useful tool in an enterprise.


Future lines of work include conducting a deeper set of experiments trying to test the generalisation power of the method,
maybe by considering bigger data divisions, bigger data sets (from a whole day, or a week), or by adding some kind
of `noise' to the dataset.
Moreover, considering the good classification results obtained in this work, the next step could be the application
of our methodology in the real system from which data was gathered, counting with the opinion of expert CSOs, in order
to know the real value of the proposal.

The study of other classification methods could be another research branch, along with the implementation of a Genetic
Programming approach, which could deal with the unbalance problem. This can be done by using a modification of the
cost associated to misclassifying patterns as done  in \cite{cost_adjustment_07}.

Finally, we also aim at extracting additional information from the URL string. This information could be transformed
into additional features that could be more discriminative than the current set of obtained rules. Moreover, a data
process involving grouping data into sessions (such as number of requests per client, or average time connection)
will be also considered.


\section*{Acknowledgements}
The authors would like to thank GENIL-SSV'2015 for ensuring the visit of Dr. Zeineb Chelly to be part of this project.  We thank Dr. Zeineb Chelly from Institut Sup\'erieur de Gestion, Tunisia for her technical insight, recommendations and suggestions and for her assistance during the practical experiments. This paper has been funded in part by European project MUSES (FP7-318508), along with Spanish National project TIN2011-28627-C04-02 (ANYSELF), project P08-TIC-03903 (EVORQ) awarded by the Andalusian Regional Government, and projects 83 (CANUBE), and GENIL PYR-2014-17, both awarded by the CEI-BioTIC UGR.

\bibliographystyle{splncs03}
\bibliography{data_mining_urls}


\end{document}
